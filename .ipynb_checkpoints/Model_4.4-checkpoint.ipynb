{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BA 545 Course Project 2: Machine Team 4 \n",
    "#### Online Shoppers Purchasing Intentions\n",
    "##### Michael Disanto, Dawn Massey PhD CPA CGMA, Brian Nicholls\n",
    "###### Final Models (4) - Spring 2020\n",
    "----------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Table of Contents:**\n",
    "\n",
    "0. [Preparing for Analyisis](#part4.0)\n",
    "1. [Initial Review and Vizualization of the Data](#part4.1)\n",
    "2. [Process the Data for Initial Data Models](#part4.2)\n",
    "3. [Split and Run the Final Models with Selected Features and Optimized hyperperameters](#part4.3)\n",
    "4. [Evaluate all the methods and compare using Ensambel Ranking](#part4.4)\n",
    "5. [Run and Evaluate the Final Models with Over Sampled data using SMOTE](#part4.5)\n",
    "6. [Run and Evaluate the Final Models using TPOT](#part4.6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preparing for Analysis  <a name=\"part4.0\"></a>\n",
    "####  Import the necesary packages for reading, analyzing, tidying, medeling, & evaluating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TO USE FOR ENTIRE TEAM\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from pandas_profiling import ProfileReport\n",
    "# import statsmodels.api as sm\n",
    "# from scipy import stats\n",
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding Items not in Fairfield Jupiter Lab\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas_profiling import ProfileReport\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necesary packages for reading, analyzing, tidying, modeling, & evaluating the data \n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "# scaler = StandardScaler().fit(X_train) >>> standardized_X = scaler.transform(X_train) >>> standardized_X_test = scaler.transform(X_test)\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "# scaler = Normalizer().fit(X_train) >>> normalized_X = scaler.transform(X_train) >>> normalized_X_test = scaler.transform(X_test)\n",
    "\n",
    "from sklearn.preprocessing import Binarizer \n",
    "# binarizer = Binarizer(threshold=0.0).fit(X) >>> binary_X = binarizer.transform(X)\n",
    "\n",
    "# Encoding Categorical Features\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# enc = LabelEncoder()\n",
    "# y = enc.fit_transform(y)\n",
    "\n",
    "# Imputation\n",
    "from sklearn.impute import (SimpleImputer, KNNImputer, MissingIndicator)\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "# imp = Imputer(missing_values=0, strategy='mean', axis=0) >>> imp.fit_transform(X_train)\n",
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "# poly = PolynomialFeatures(5) >>> poly.fit_transform(X)\n",
    "\n",
    "# Train/Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=0)\n",
    "\n",
    "# Various Models\n",
    "from sklearn.cluster import KMeans\n",
    "# k_means = KMeans(n_clusters=3, random_state=0\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=0.95)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# logreg = LogisticRegression()\n",
    "\n",
    "from sklearn.linear_model import RidgeCV\n",
    "# rrm = RidgeCV(alphas=(0.01, 0.1, 1.0, 10.0), normalize=True)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# gnb = GaussianNB()\n",
    "\n",
    "from sklearn.svm import SVC \n",
    "# svc = SVC(kernel='linear')\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# lr = LinearRegression(normalize=True)\n",
    "\n",
    "from sklearn import neighbors\n",
    "# knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "## Fit the model\n",
    "# # Supervised learning\n",
    "# lr.fit(X, y)\n",
    "# knn.fit(X_train, y_train)\n",
    "# svc.fit(X_train, y_train)   \n",
    "\n",
    "# #Unsupervised Learning \n",
    "# k_means.fit(X_train) \n",
    "# pca_model = pca.fit_transform(X_train)\n",
    "\n",
    "## Predict Y\n",
    "# Supervised Estimators\n",
    "# y_pred = svc.predict(np.random.random((2,5))) \n",
    "# y_pred = lr.predict(X_test)\n",
    "# y_pred = knn.predict_proba(X_test)   \n",
    "# Unsupervised Estimators \n",
    "# y_pred = k_means.predict(X_test)\n",
    "\n",
    "# Packages to evaluate Model Performance (Classification)\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "# print(classification_report(y_test_log, y_pred_log))\n",
    "\n",
    "# Packages to evaluate Model Performance (Linear)\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "# y_true = [3, -0.5, 2] >>> mean_absolute_error(y_true, y_pred)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# mean_squared_error(y_test, y_pred)\n",
    "from sklearn.metrics import r2_score \n",
    "# r2_score(y_true, y_pred)\n",
    "\n",
    "#from sklearn.cross_validation import cross_val_score \n",
    "# print(cross_val_score(knn, X_train, y_train, cv=4)) >>> print(cross_val_score(lr, X, y, cv=2)\n",
    "\n",
    "# Packages for Over/Under Sampling\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Initial review and vizualization of the data  <a name=\"part4.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in our original data and a copy for the base model\n",
    "df = pd.read_csv('online_shoppers_intention.csv', na_values=r'-')\n",
    "df_base = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the datatypes and descriptive statistics of the dataset\n",
    "display(df_base.info())\n",
    "display(df_base.describe())\n",
    "print(' ')\n",
    "display(df_base.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Heatmap for the dataframe\n",
    "spearman =df.corr(method ='spearman')\n",
    "plt.figure(figsize=(25,10))\n",
    "sns.heatmap(spearman, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Process the data for Initial Data Models  <a name=\"part4.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2.0 Pipeline [Prep] - Create subsets and list to be used later in the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Continuous Variable List\n",
    "df_continuous = df.iloc[:,:9]\n",
    "continuous_df_list = df_continuous.columns.tolist()\n",
    "\n",
    "# Create Categorical Variable List\n",
    "df_categorical = df.iloc[:,9:-1]\n",
    "categorical_df_list = df_categorical.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2.1 Pipeline [Impute] - Initial Imputation of Categorical Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline - Initial Imputation of Categorical Features:\n",
    "\n",
    "## Replace the VisitorType 'Other' with the variable's mode, namely: 'Returning_Visitor'\n",
    "df['VisitorType'] = df['VisitorType'].replace('Other','Returning_Visitor')\n",
    "df.groupby('VisitorType').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Revenue before splitting the data to allow for modeling\n",
    "\n",
    "enc = LabelEncoder()\n",
    "\n",
    "df['VisitorType'] = enc.fit_transform(df['VisitorType'])\n",
    "df['Weekend'] = enc.fit_transform(df['Weekend'])\n",
    "df['Revenue'] = enc.fit_transform(df['Revenue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline - Binning Categorical Features:\n",
    "\n",
    "def holiday_bin_func(month) :\n",
    "    if month == 'May':\n",
    "        return int(1)\n",
    "    elif month == 'Feb':\n",
    "        return int(1)\n",
    "    else:\n",
    "        return int(0)\n",
    "    \n",
    "df['Holiday_Bin'] = df['Month'].apply(holiday_bin_func)\n",
    "\n",
    "##Reduce months to the top 4 in which there are transactions and \"other\"\n",
    "def month_bin_func(month) :\n",
    "    if month == 'May':\n",
    "        return int(5)\n",
    "    elif month == 'Nov':\n",
    "        return int(11)\n",
    "    elif month == 'Mar':\n",
    "        return int(3)\n",
    "    elif month == 'Dec':\n",
    "        return int(12)\n",
    "    else:\n",
    "        return int(0)\n",
    "    \n",
    "df['Month_Bin'] = df['Month'].apply(month_bin_func)\n",
    "\n",
    "\n",
    "##Encode month names to numerical representations\n",
    "\n",
    "def month_func(month) :\n",
    "    if month == 'Jan':\n",
    "        return int(1)\n",
    "    elif month == 'Feb':\n",
    "        return int(2)\n",
    "    elif month == 'Mar':\n",
    "        return int(3)\n",
    "    elif month == 'Apr':\n",
    "        return int(4)\n",
    "    elif month == 'May':\n",
    "        return int(5)\n",
    "    elif month == 'June':\n",
    "        return int(6)\n",
    "    elif month == 'Jul':\n",
    "        return int(7)\n",
    "    elif month == 'Aug':\n",
    "        return int(8)\n",
    "    elif month == 'Sep':\n",
    "        return int(9)\n",
    "    elif month == 'Oct':\n",
    "        return int(10)\n",
    "    elif month == 'Nov':\n",
    "        return int(11)\n",
    "    elif month == 'Dec':\n",
    "        return int(12)\n",
    "\n",
    "df['Month'] = df['Month'].apply(month_func)\n",
    "\n",
    "def month_quarterly_func(month) :\n",
    "    if month <= 3:\n",
    "        return int(1)\n",
    "    elif month >= 4 and month <= 6:\n",
    "        return int(2)\n",
    "    elif month >= 7 and month <= 9:\n",
    "        return int(3)\n",
    "    elif month >= 10 and month <= 12:\n",
    "        return int(4)\n",
    "\n",
    "df['Quarter'] = df['Month'].apply(month_quarterly_func)\n",
    "\n",
    "##Reduce categories for Operating Systems to the top 3 plus \"other\"\n",
    "### Operating Systems – is a categorical variable and most of the data (~95%) is in one of three operating systems (2, 1, 3). \n",
    "def binning_operating_systems(B):\n",
    "    if (B <= 3):\n",
    "        return(B)\n",
    "    else:\n",
    "        return(4) # creating a category of 4 for all Operating Systems > 3\n",
    "\n",
    "df['OperatingSystems_Bin']=df['OperatingSystems'].apply(binning_operating_systems)   # Creating a new column in the df\n",
    "\n",
    "\n",
    "      \n",
    "##Reduce categories for Browser to the top 3 plus \"other\"\n",
    "### Browser – is a categorical variable and most the data (~91%) come from three browsers (2, 1, 4).\n",
    "def binning_browser(B):\n",
    "    if (B == 3) or (B > 4): \n",
    "        return(3) \n",
    "    else:\n",
    "        return(B) \n",
    "\n",
    "df['Browser_Bin']=df['Browser'].apply(binning_browser)   # Creating a new column in the df\n",
    "      \n",
    "      \n",
    "##Reduce categories for Region to the top 4 plus \"other\"\n",
    "### Region – is a categorical variable for region from which the visitor came. The top four account for ~77% of the data (i.e., region 1, 3, 4, 2).\n",
    "def binning_region(B):\n",
    "    if (B <= 4):\n",
    "        return(B)\n",
    "    else:\n",
    "        return(5) # creating a category of 5 for all Regions > 4\n",
    "\n",
    "df['Region_Bin']=df['Region'].apply(binning_region)   # Creating a new column in the df\n",
    "\n",
    "      \n",
    "      \n",
    "##Reduce categories for TrafficType to the top 3 plus \"other\"\n",
    "### TrafficType – is a categorical variable to indicate how visitor arrived at website. The top three account for approximately 67% of the referrals (i.e., types 2, 1, 3).\n",
    "def binning_traffic_type(B):\n",
    "    if (B <= 3):\n",
    "        return(B)\n",
    "    else:\n",
    "        return(4) # creating a category of 4 for all Traffic Types > 3\n",
    "\n",
    "df['TrafficType_Bin']=df['TrafficType'].apply(binning_traffic_type)   # Creating a new column in the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add creates features / bins to categorical list\n",
    "added_categorical_features = df.iloc[:,-7:]\n",
    "added_categorical_list = added_categorical_features.columns.tolist()\n",
    "\n",
    "##Combine added features list with categorical list\n",
    "categorical_df_list = categorical_df_list + added_categorical_list\n",
    "\n",
    "##Check work by printing combined categorical list\n",
    "categorical_df_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2.2 Pipeline [Transform] - Initial Log Transformation Continuous Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute zeros to one (by adding one to all values) before applying the log transformation\n",
    "\n",
    "# Impute Zeros before doing the log\n",
    "for column in continuous_df_list:\n",
    "    df[column] = df[column] + 1\n",
    "\n",
    "# Check imputation by printing a samplle of rows\n",
    "display(df[continuous_df_list].sample(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2.3 Pipeline [Calcualated Features] - Initial Feature Engineering of continuous features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform initial log transformation on PageValues\n",
    "df['PageValues_Log'] = np.log(df['PageValues'])\n",
    "\n",
    "# Create features combining ('ExitRates' and 'BounceRates'), which are highly correlated with each other and the target\n",
    "df['Bounce_Exit_Rate_Avg'] = (df['BounceRates'] + df['ExitRates'])/2\n",
    "df['Bounce_Exit_Rate_WeightedAvg'] = ((df['BounceRates']*.6) + (df['ExitRates']*.4))\n",
    "df['Bounce_Exit_Rate_Avg_PageVales'] = (df['Bounce_Exit_Rate_Avg'] / df['PageValues']) # add a chart to compare the trends and distributions\n",
    "df['Bounce_per_Exit_Rate'] = df['BounceRates'] / df['ExitRates']\n",
    "\n",
    "# Create features combining ('Administrative' and 'Administrative_Duration') and ('ExitRates' and 'BounceRates'), which are highly correlated with each other and the target\n",
    "df['Admin_per_Exit'] = df['Administrative'] / df['ExitRates'] # average the denominator 'Bounce_Exit_Rate_Avg'\n",
    "df['Admin_per_Bounce'] = df['Administrative'] / df['BounceRates'] # average the denominator 'Bounce_Exit_Rate_Avg'\n",
    "df['Admin_per_bounce_exit_avg'] = df['Administrative'] / df['Bounce_Exit_Rate_Avg'] # average the denominator 'Bounce_Exit_Rate_Avg'\n",
    "\n",
    "df['Admin_dur_per_Exit'] = df['Administrative_Duration'] / df['ExitRates'] # average the denominator 'Bounce_Exit_Rate_Avg'\n",
    "df['Admin_dur_per_Bounce'] = df['Administrative_Duration'] / df['BounceRates'] # average the denominator 'Bounce_Exit_Rate_Avg'\n",
    "df['Admin_dur_per_bounce_exit_avg'] = df['Administrative_Duration'] / df['Bounce_Exit_Rate_Avg'] # average the denominator 'Bounce_Exit_Rate_Avg'\n",
    "\n",
    "\n",
    "# Create features combining ('Informational' and 'Informational_Duration') and, ('ExitRates' and 'BounceRates'), which are highly correlated with each other and the target\n",
    "df['Info_per_Exit'] = df['Informational'] / df['ExitRates'] # average the denominator 'Bounce_Exit_Rate_Avg'\n",
    "df['Info_per_Bounce'] = df['Informational'] / df['BounceRates'] # average the denominator 'Bounce_Exit_Rate_Avg'\n",
    "df['Info_per_bounce_exit_avg'] = df['Informational'] / df['Bounce_Exit_Rate_Avg'] # average the denominator 'Bounce_Exit_Rate_Avg'\n",
    "\n",
    "df['Info_dur_per_Exit'] = df['Informational_Duration'] / df['ExitRates'] # average the denominator 'Bounce_Exit_Rate_Avg'\n",
    "df['Info_dur_per_Bounce'] = df['Informational_Duration'] / df['BounceRates'] # average the denominator 'Bounce_Exit_Rate_Avg'\n",
    "df['Info_dur_per_bounce_exit_avg'] = df['Informational_Duration'] / df['Bounce_Exit_Rate_Avg'] # average the denominator 'Bounce_Exit_Rate_Avg'\n",
    "\n",
    "\n",
    "# Create features combining ('ProductRelated' and 'ProductRelated_Duration') and, ('ExitRates' and 'BounceRates'), which are highly correlated with each other and the target\n",
    "df['Product_per_Exit'] = df['ProductRelated'] / df['ExitRates'] # average the denominator 'Bounce_Exit_Rate_Avg'\n",
    "df['Product_per_Bounce'] = df['ProductRelated'] / df['BounceRates'] # average the denominator 'Bounce_Exit_Rate_Avg'\n",
    "df['Prod_per_bounce_exit_avg'] = df['ProductRelated'] / df['Bounce_Exit_Rate_Avg'] # average the denominator 'Bounce_Exit_Rate_Avg'\n",
    "\n",
    "df['Product_dur_per_Exit'] = df['ProductRelated_Duration'] / df['ExitRates'] # average the denominator 'Bounce_Exit_Rate_Avg'\n",
    "df['Product_dur_per_Bounce'] = df['ProductRelated_Duration'] / df['BounceRates'] # average the denominator 'Bounce_Exit_Rate_Avg\n",
    "df['Prod_dur_per_bounce_exit_avg'] = df['ProductRelated_Duration'] / df['Bounce_Exit_Rate_Avg'] # average the denominator 'Bounce_Exit_Rate_Avg'\n",
    "\n",
    "\n",
    "# Create features combining ('Administrative_Duration','Informational_Duration' and 'ProductRelated_Duration'), which are highly correlated with each other and the target\n",
    "df['Total_Duration'] = df['Administrative_Duration'] + df['Informational_Duration'] + df['ProductRelated_Duration']\n",
    "df['Total_Duration_Avg'] = (df['Total_Duration'])/3\n",
    "df['Admin_Duration_percent_TotalDuration'] = df['Administrative_Duration'] / df['Total_Duration']\n",
    "df['Info_Duration_percent_TotalDuration'] = df['Informational_Duration'] / df['Total_Duration']\n",
    "df['Product_Duration_percent_TotalDuration'] = df['ProductRelated_Duration'] / df['Total_Duration']\n",
    "\n",
    "\n",
    "# Create features related to 'Total_Duration' as compared to ('ExitRates', 'BounceRates' and a combination of the two), which are highly correlated with each other and the target\n",
    "df['TotalDuration_per_ExitRates'] = df['Total_Duration'] / df['ExitRates']\n",
    "df['TotalDuration_per_BounceRates'] = df['Total_Duration'] / df['BounceRates']\n",
    "df['TotalDuration_per_BounceExit_avg'] = df['Total_Duration'] / df['Bounce_Exit_Rate_Avg']\n",
    "\n",
    "\n",
    "#Create features using 'PageValues', which is highly correlated with the target,as the denominator\n",
    "df['Admin_per_PageValues'] = df['Administrative'] / df['PageValues']\n",
    "df['Admin_Duration_per_PageValues'] = df['Administrative_Duration'] / df['PageValues']\n",
    "df['Informational_per_PageValues'] = df['Informational'] / df['PageValues']\n",
    "df['Info_Duration_per_PageValues'] = df['Informational_Duration'] / df['PageValues']\n",
    "df['ProductRelated_per_PageValues'] = df['ProductRelated'] / df['PageValues']\n",
    "df['Product_Duration_per_PageValues'] = df['ProductRelated_Duration'] / df['PageValues']\n",
    "df['TotalDuration_per_PageValues'] = df['Total_Duration'] / df['PageValues']\n",
    "df['Exit_per_PageValues'] = df['ExitRates'] / df['PageValues']\n",
    "df['Bounce_per_PageValues'] = df['BounceRates'] / df['PageValues']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add transformed / calculated features to continuous list\n",
    "\n",
    "##Create added features list (to include PageValues_Log)\n",
    "added_continuous_features = df.iloc[:,-40:]\n",
    "added_continuous_list = added_continuous_features.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Combine added features list with continuous list\n",
    "continuous_df_list = continuous_df_list + added_continuous_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Check work by printing a list of the added continouous columns\n",
    "continuous_df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the created columns in a sample\n",
    "new_continuous_df = df.loc[:,continuous_df_list]\n",
    "y = df.loc[:,'Revenue']\n",
    "display(new_continuous_df.sample(20))\n",
    "display(' ')\n",
    "new_categorical_df = df.loc[:,categorical_df_list]\n",
    "display(new_categorical_df.sample(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2.4 Pipeline [Middle] - Min Max Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the attributes that have a range outside of zero to one (0 - 1)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scale = scaler.fit(new_continuous_df)\n",
    "scaled = scaler.transform(new_continuous_df)\n",
    "scaled_continuous_df = pd.DataFrame(scaled,columns = continuous_df_list)\n",
    "df[continuous_df_list] = scaled_continuous_df[continuous_df_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2.5 Pipeline [Middle] - Normalize the data to address skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initially displaying the skewness of all attributes\n",
    "skew_df = pd.DataFrame(df[continuous_df_list].skew().abs())\n",
    "\n",
    "#filter skew attributes by absolute values of 0.5\n",
    "skew_over = skew_df[(skew_df > 0.5).any(axis=1)]\n",
    "skew_cols = skew_over.index.tolist()\n",
    "display(skew_cols)\n",
    "display(len(skew_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of columns to adjust for skewness\n",
    "\n",
    "for i in skew_cols:\n",
    "    df[i+'_skew'] = df[i]\n",
    "    \n",
    "cols_to_skew = df.iloc[:,-len(skew_cols):].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_skew_cols = cols_to_skew.tolist()\n",
    "continuous_df_list = continuous_df_list + continuous_skew_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the columns that have skewness using quantile_transform\n",
    "\n",
    "from sklearn.preprocessing import quantile_transform\n",
    "transformed_qt = quantile_transform(df[cols_to_skew],random_state=0,copy=True)\n",
    "transformed_qt_df = pd.DataFrame(transformed_qt,columns = cols_to_skew)\n",
    "df[cols_to_skew] = transformed_qt_df[cols_to_skew]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display skewness after quantile_transform\n",
    "display(df[cols_to_skew].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of columns that still need to be adjusted for skewness (i.e., after applying quantile_transform)\n",
    "Still_skew_df = pd.DataFrame(df[cols_to_skew].skew().abs())\n",
    "\n",
    "##Filter skew attributes by an absolute value of 0.5\n",
    "still_skew_over = Still_skew_df[(Still_skew_df > 0.5).any(axis=1)]\n",
    "still_skew_list =(still_skew_over.index.tolist())\n",
    "display(still_skew_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the remaining columns that continue to have skewness (after applying quantile_transform) using PowerTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "transform = pt.fit(df[still_skew_list])\n",
    "transformed = pt.transform((df[still_skew_list]))\n",
    "transformed_df = pd.DataFrame(transformed,columns = still_skew_list)\n",
    "df[still_skew_list] = transformed_df[still_skew_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for remaining skewness\n",
    "display(df[still_skew_list].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman_possible =df[continuous_df_list].corrwith(df['Revenue'])\n",
    "spearman_possible.head(60)\n",
    "# plt.figure(figsize=(30,15))\n",
    "# sns.heatmap(spearman_possible, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = pd.DataFrame(df[continuous_df_list].corrwith(df['Revenue']).abs())\n",
    "\n",
    "#filter skew attributes by absolute values of 0.20\n",
    "corr_over = corr_df[(corr_df > 0.20).any(axis=1)]\n",
    "#display(corr_over.index)\n",
    "\n",
    "continuous_model_cols = corr_over.index.tolist()\n",
    "display(continuous_model_cols)\n",
    "display(len(continuous_model_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determined possible features list by removing corresponding, duplicative features that were not adjusted for skewness\n",
    " \n",
    "possible_Features_list =['ExitRates',\n",
    " 'PageValues',\n",
    " 'PageValues_Log',\n",
    " 'Bounce_Exit_Rate_Avg_PageVales',\n",
    " 'Informational_per_PageValues',\n",
    " 'Exit_per_PageValues',\n",
    " 'Bounce_per_PageValues',\n",
    " 'ProductRelated_Duration_skew',\n",
    " 'ExitRates_skew',\n",
    " 'PageValues_skew',\n",
    " 'PageValues_Log_skew',\n",
    " 'Bounce_Exit_Rate_Avg_skew',\n",
    " 'Bounce_Exit_Rate_WeightedAvg_skew',\n",
    " 'Bounce_Exit_Rate_Avg_PageVales_skew',\n",
    " 'Admin_per_Exit_skew',\n",
    " 'Admin_per_bounce_exit_avg_skew',\n",
    " 'Admin_dur_per_Exit_skew',\n",
    " 'Admin_dur_per_bounce_exit_avg_skew',\n",
    " 'Info_per_Exit_skew',\n",
    " 'Info_per_bounce_exit_avg_skew',\n",
    " 'Info_dur_per_Exit_skew',\n",
    " 'Info_dur_per_bounce_exit_avg_skew',\n",
    " 'Product_per_Exit_skew',\n",
    " 'Product_per_Bounce_skew',\n",
    " 'Prod_per_bounce_exit_avg_skew',\n",
    " 'Product_dur_per_Exit_skew',\n",
    " 'Product_dur_per_Bounce_skew',\n",
    " 'Prod_dur_per_bounce_exit_avg_skew',\n",
    " 'Total_Duration_skew',\n",
    " 'Total_Duration_Avg_skew',\n",
    " 'TotalDuration_per_ExitRates_skew',\n",
    " 'TotalDuration_per_BounceRates_skew',\n",
    " 'TotalDuration_per_BounceExit_avg_skew',\n",
    " 'Admin_per_PageValues_skew',\n",
    " 'Informational_per_PageValues_skew',\n",
    " 'Info_Duration_per_PageValues_skew',\n",
    " 'ProductRelated_per_PageValues_skew',\n",
    " 'Product_Duration_per_PageValues_skew',\n",
    " 'TotalDuration_per_PageValues_skew',\n",
    " 'Exit_per_PageValues_skew',\n",
    " 'Bounce_per_PageValues_skew','Revenue']    \n",
    "possible_Features_list.sort()\n",
    "possible_Features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_possible_Features_list = ['Admin_dur_per_Exit_skew',\n",
    " 'Admin_dur_per_bounce_exit_avg_skew',\n",
    " 'Admin_per_Exit_skew',\n",
    " 'Admin_per_PageValues_skew',\n",
    " 'Admin_per_bounce_exit_avg_skew',\n",
    " 'Bounce_Exit_Rate_Avg_PageVales',\n",
    " 'Bounce_Exit_Rate_Avg_PageVales_skew',\n",
    " 'Bounce_Exit_Rate_Avg_skew',\n",
    " 'Bounce_Exit_Rate_WeightedAvg_skew',\n",
    " 'Bounce_per_PageValues',\n",
    " 'Bounce_per_PageValues_skew',\n",
    " 'ExitRates',\n",
    " 'ExitRates_skew',\n",
    " 'Exit_per_PageValues',\n",
    " 'Exit_per_PageValues_skew',\n",
    " 'Info_Duration_per_PageValues_skew',\n",
    " 'Info_dur_per_Exit_skew',\n",
    " 'Info_dur_per_bounce_exit_avg_skew',\n",
    " 'Info_per_Exit_skew',\n",
    " 'Info_per_bounce_exit_avg_skew',\n",
    " 'Informational_per_PageValues',\n",
    " 'Informational_per_PageValues_skew',\n",
    " 'PageValues',\n",
    " 'PageValues_Log',\n",
    " 'PageValues_Log_skew',\n",
    " 'PageValues_skew',\n",
    " 'Prod_dur_per_bounce_exit_avg_skew',\n",
    " 'Prod_per_bounce_exit_avg_skew',\n",
    " 'ProductRelated_Duration_skew',\n",
    " 'ProductRelated_per_PageValues_skew',\n",
    " 'Product_Duration_per_PageValues_skew',\n",
    " 'Product_dur_per_Bounce_skew',\n",
    " 'Product_dur_per_Exit_skew',\n",
    " 'Product_per_Bounce_skew',\n",
    " 'Product_per_Exit_skew',\n",
    " 'TotalDuration_per_BounceExit_avg_skew',\n",
    " 'TotalDuration_per_BounceRates_skew',\n",
    " 'TotalDuration_per_ExitRates_skew',\n",
    " 'TotalDuration_per_PageValues_skew',\n",
    " 'Total_Duration_Avg_skew',\n",
    " 'Total_Duration_skew',\n",
    " 'Revenue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Heatmap for the dataframe\n",
    "possible_Corr_df = df.loc[:,sorted_possible_Features_list]\n",
    "\n",
    "spearman_possible =possible_Corr_df.corr(method= 'spearman')\n",
    "plt.figure(figsize=(40,15))\n",
    "sns.heatmap(spearman_possible, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered out intercorrelated featues for feature importance\n",
    "continuous_feature_selection = ['Admin_per_Exit_skew','Admin_per_PageValues_skew', 'Bounce_Exit_Rate_Avg_skew','Bounce_per_PageValues_skew','Exit_per_PageValues_skew','Informational_per_PageValues_skew','Info_per_Exit_skew','PageValues_Log_skew',\n",
    "'Product_per_Exit_skew','ProductRelated_Duration_skew','ProductRelated_per_PageValues_skew','TotalDuration_per_BounceRates_skew','TotalDuration_per_PageValues_skew','Total_Duration_skew']\n",
    "\n",
    "display(len(continuous_feature_selection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on the above: to be updated\n",
    "    - Need to choose only one of the variables for which there is a corresponding duration variable - namely: \n",
    "    - Administrative_skew or Administrative_Duration_skew (correlation 0.94); initial decision is to utilize Administrative_skew as it is more highly correlated with the target variable\n",
    "    - Informational_skew or Informational_Duration_skew (correlation 0.95); initial decision is to utilize Informational_skew because Informational_Duration_skew is incorporated into the Total_Duration variables (discussed below)\n",
    "    - ProductRelated_skew or Product_RelatedDuration_skew (correlation 0.88); initial decision is to utilize ProductRelated_Duration_skew as it is more highly correlated with the target variable\n",
    "\n",
    "    - Need to choose only one of the variables that are 100% corelated – namely:\n",
    "    - The PageValues variables (since they all are 100% correlated); initial decision is to use PagesValues_skew (rather than PageValues_Log_skew or PageValues_Log10_skew) because the “raw” feature is more interpretable than its logged counterparts\n",
    "    - Total_Duration_skew and Total_Duration_Avg_skew; initial decision is to forego both in favor of engineered features that incorporate total duration\n",
    "    - Bounce_Exit_Rate_Avg_skew and Bounce_Exit_Rate_WeightedAvg_skew; initial decision is to utilize the last one since it incorporates both Bounce and Exit Rates and utilizes a weighted average, which gives relative weights to the inputs\n",
    "\n",
    "    - Need to carefully consider variables containing “Bounce” and “Exit” since BounceRates_skew and ExitRates_skew are highly correlated (correlation 0.60) – namely:\n",
    "    - ExitRates_skew and Bounce_Exit_Rate_Avg_skew (correlation 0.98) as well as ExitRates_skew and  Bounce_Exit_Rate_WeightedAvg_skew (correlation 0.96); initial decision is to utilize the last one since it incorporates both Bounce and Exit Rates and utilizes a weighted                 average, which gives relative weights to the inputs\n",
    "    - Admin_per_Exit_skew and Admin_per_Bounce_skew (correlation 0.97) Initial decision is to use Admin_per Exit_skew because it is more highly correlated with the target variable.\n",
    "\n",
    "    - Exit_per_PageValues and Bounce_per_PageValues (correlation 0.82) Initial decision is to use Bounce_per_PageValues because it has fewer high correlations with the other variables \n",
    "\n",
    "**Need to choose only one of the variables from pairs that capture related information - namely:**\n",
    "    \n",
    "    - TotalDuration_per_PageValues_skew or Product_Duration_per_PageValues_skew (correlation 0.99) Initial decision is to use Product_Duration_per_PageValues_skew because it is more highly correlated with the target variable.\n",
    "    - Admin_per_PageValues_skew and AdminDuration_per_PageValues_skew (correlation 0.76); initial decision is to include Admin_per_PageValues_skew since it is much more highly correlated with y\n",
    "\n",
    "**Additionally, we recommend including**\n",
    "   \n",
    "    - Month because retail businesses are seasonal; and\n",
    "    - VisitorType because of the importance of customer loyalty in a retail environment\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical Features Manual Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_corr_df = pd.DataFrame(df[categorical_df_list].corrwith(df['Revenue']).abs())\n",
    "\n",
    "#filter skew attributes by absolute values of 0.001\n",
    "cat_corr_over = cat_corr_df[(cat_corr_df > 0.0).any(axis=1)]\n",
    "\n",
    "categorical_model_cols = cat_corr_over.index.tolist()\n",
    "display(categorical_model_cols)\n",
    "display(len(categorical_model_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_corr_df\n",
    "Cat_feature_selection = cat_corr_df.index.tolist()\n",
    "Cat_feature_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Heatmap for the dataframe\n",
    "cat_Corr_df = df.loc[:,categorical_df_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_spearman_possible =cat_Corr_df.corr(method ='spearman')\n",
    "plt.figure(figsize=(30,15))\n",
    "sns.heatmap(cat_spearman_possible, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cat_feature_selection =['SpecialDay','VisitorType','Weekend','Holiday_Bin','Month_Bin','Quarter','OperatingSystems_Bin','Browser_Bin','Region_Bin','TrafficType_Bin']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on the above: needs to be updated\n",
    "    - Need to choose only one of the variables for which there is a corresponding duration variable - namely: \n",
    "    - Administrative_skew or Administrative_Duration_skew (correlation 0.94); initial decision is to utilize Administrative_skew as it is more highly correlated with the target variable\n",
    "    - Informational_skew or Informational_Duration_skew (correlation 0.95); initial decision is to utilize Informational_skew because Informational_Duration_skew is incorporated into the Total_Duration variables (discussed below)\n",
    "    - ProductRelated_skew or Product_RelatedDuration_skew (correlation 0.88); initial decision is to utilize ProductRelated_Duration_skew as it is more highly correlated with the target variable\n",
    "\n",
    "    - Need to choose only one of the variables that are 100% corelated – namely:\n",
    "    - The PageValues variables (since they all are 100% correlated); initial decision is to use PagesValues_skew (rather than PageValues_Log_skew or PageValues_Log10_skew) because the “raw” feature is more interpretable than its logged counterparts\n",
    "    - Total_Duration_skew and Total_Duration_Avg_skew; initial decision is to forego both in favor of engineered features that incorporate total duration\n",
    "    - Bounce_Exit_Rate_Avg_skew and Bounce_Exit_Rate_WeightedAvg_skew; initial decision is to utilize the last one since it incorporates both Bounce and Exit Rates and utilizes a weighted average, which gives relative weights to the inputs\n",
    "\n",
    "    - Need to carefully consider variables containing “Bounce” and “Exit” since BounceRates_skew and ExitRates_skew are highly correlated (correlation 0.60) – namely:\n",
    "    - ExitRates_skew and Bounce_Exit_Rate_Avg_skew (correlation 0.98) as well as ExitRates_skew and  Bounce_Exit_Rate_WeightedAvg_skew (correlation 0.96); initial decision is to utilize the last one since it incorporates both Bounce and Exit Rates and utilizes a weighted                 average, which gives relative weights to the inputs\n",
    "    - Admin_per_Exit_skew and Admin_per_Bounce_skew (correlation 0.97) Initial decision is to use Admin_per Exit_skew because it is more highly correlated with the target variable.\n",
    "\n",
    "    - Exit_per_PageValues and Bounce_per_PageValues (correlation 0.82) Initial decision is to use Bounce_per_PageValues because it has fewer high correlations with the other variables \n",
    "\n",
    "**Need to choose only one of the variables from pairs that capture related information - namely:**\n",
    "    \n",
    "    - TotalDuration_per_PageValues_skew or Product_Duration_per_PageValues_skew (correlation 0.99) Initial decision is to use Product_Duration_per_PageValues_skew because it is more highly correlated with the target variable.\n",
    "    - Admin_per_PageValues_skew and AdminDuration_per_PageValues_skew (correlation 0.76); initial decision is to include Admin_per_PageValues_skew since it is much more highly correlated with y\n",
    "\n",
    "**Additionally, we recommend including**\n",
    "   \n",
    "    - Month because retail businesses are seasonal; and\n",
    "    - VisitorType because of the importance of customer loyalty in a retail environment\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered out intercorrelated featues for feature importance\n",
    "feature_importance_list = Cat_feature_selection + continuous_feature_selection\n",
    "display(feature_importance_list,len(feature_importance_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Split and run the Final Models with Selected Features and Optimized Hyperperameters  <a name=\"part4.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a full list of continuous and categorical features based on aditional data preparation \n",
    "continuous_categorical_list = feature_importance_list\n",
    "\n",
    "# Specify features (X) and target (Y) in dataset\n",
    "X,y = df.loc[:,continuous_categorical_list],df.loc[:,'Revenue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resplit the data based on additional data preparation completed post initial split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,random_state=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Naive Bayes Model (NB)\n",
    ">   <b> Accuracy: </b> 87%\n",
    "<br><b> AUC: </b> 87%\n",
    "<br><b> Next Steps: </b> Additional parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance : Forward Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://towardsdatascience.com/feature-selection-using-wrapper-methods-in-python-f0d352b346f\n",
    "\n",
    "def forward_selection(data, target, significance_level=0.05):\n",
    "    initial_features = data.columns.tolist()\n",
    "    best_features = []\n",
    "    while (len(list(initial_features))>0):\n",
    "        remaining_features = list(set(initial_features)-set(best_features))\n",
    "        new_pval = pd.Series(index=remaining_features)\n",
    "        for new_column in remaining_features:\n",
    "            model = sm.OLS(target, sm.add_constant(data[best_features+[new_column]])).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        min_p_value = new_pval.min()\n",
    "        if(min_p_value<significance_level):\n",
    "            best_features.append(new_pval.idxmin())\n",
    "        else:\n",
    "            break\n",
    "    return best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = forward_selection(X_train, y_train, significance_level=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify/Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Gaussian Classifier\n",
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using the training sets\n",
    "gnb.fit(X_train[best_features], y_train)\n",
    "\n",
    "# Predict the response for testing data\n",
    "y_NB_pred = gnb.predict(X_test[best_features])\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"y Accuracy:\",metrics.accuracy_score(y_test, y_NB_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification report\n",
    "class_names=[0,1] # name  of classes\n",
    "print(metrics.classification_report(y_test, y_NB_pred, labels=class_names))\n",
    "\n",
    "# Compute AUC\n",
    "y_NB_pred_proba = gnb.predict_proba(X_test[best_features])[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_NB_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_NB_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "print(\"\")\n",
    "\n",
    "# Create Confusion Matrix/Heatmap\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_NB_pred)\n",
    "cnf_matrix\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Decision Tree Model (DT)\n",
    ">   <b> Accuracy: </b> 85%\n",
    "<br><b> AUC: </b> 69%\n",
    "<br><b> Next Steps: </b> Additional parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection: SelectFromModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "embeded_dt_selector = SelectFromModel(DecisionTreeClassifier(), max_features=10)\n",
    "embeded_dt_selector.fit(X_train, y_train)\n",
    "embeded_dt_support = embeded_dt_selector.get_support()\n",
    "embeded_dt_feature = X_train.loc[:,embeded_dt_support].columns.tolist()\n",
    "print(str(len(embeded_dt_feature)), 'selected features')\n",
    "print(embeded_dt_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparamerter Optimization: GridSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add in GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Define the grid\n",
    "criterion=['gini','entropy']\n",
    "splitter=['best','random']\n",
    "max_depth=[2,3,4,5]\n",
    "min_samples_split = [2,3]\n",
    "param_grid_dt = dict(criterion=criterion, splitter=splitter,max_depth=max_depth,min_samples_split=min_samples_split)\n",
    "\n",
    "#Run the grid search\n",
    "#import time\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "grid_dt = GridSearchCV(estimator=dt, param_grid=param_grid_dt, cv = 10, n_jobs=-1)\n",
    "\n",
    "#start_time = time.time()\n",
    "grid_result_dt = grid_dt.fit(X_train[embeded_dt_feature], y_train)\n",
    "# Summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result_dt.best_score_, grid_result_dt.best_params_))\n",
    "\n",
    "#The below is for execution time - not important for us right now, so commented out\n",
    "# print(\"Execution time: \" + str((time.time() - start_time)) + ' ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign selected hyperperameters to a variable for ensamble ranking during the evaluation stage\n",
    "dt_Peram_dict = grid_result_dt.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify/Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the response for test dataset\n",
    "y_DT_pred = grid_dt.predict(X_test[embeded_dt_feature])\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_DT_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the Classification Report\n",
    "print(metrics.classification_report(y_test, y_DT_pred))\n",
    "\n",
    "#AUC for y\n",
    "y_DT_pred_proba = grid_dt.predict_proba(X_test[embeded_dt_feature])[::,1]\n",
    "fpr_DT, tpr_DT, _ = metrics.roc_curve(y_test,  y_DT_pred_proba)\n",
    "auc_DT = metrics.roc_auc_score(y_test, y_DT_pred_proba)\n",
    "plt.plot(fpr_DT,tpr_DT,label=\"auc=\"+str(auc_DT))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "print(\"\")\n",
    "\n",
    "#Print Confusion Matrix\n",
    "cnf_matrix_DT = metrics.confusion_matrix(y_test, y_DT_pred)\n",
    "cnf_matrix_DT\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_DT), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix DT', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Random Forest Model (RF)\n",
    ">   <b> Accuracy: </b> 89%\n",
    "<br><b> AUC: </b> 87%\n",
    "<br><b> Next Steps: </b> Additional parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Random Forest Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection: SelectFromModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded_rf_selector = SelectFromModel(RandomForestClassifier(), max_features=24)\n",
    "embeded_rf_selector.fit(X_train, y_train)\n",
    "embeded_rf_support = embeded_rf_selector.get_support()\n",
    "embeded_rf_feature = X_train.loc[:,embeded_rf_support].columns.tolist()\n",
    "print(str(len(embeded_rf_feature)), 'selected features')\n",
    "print(embeded_rf_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparamerter Optimization: GridSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://urldefense.com/v3/__https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74*5Cn__;JQ!!KIFmrYtlezdzESbnm_I!UCnvV89lnf4zyYniQ1tmp0cEqfro80T-cjcfwd7vRwyp_8aShQmgOyEKh8XH9GthAmgdQtSt5GubdFc$ \n",
    "## Define the grid\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [10,20,30]\n",
    "random_state = [10,20,50,100]\n",
    "criterion=['gini','entropy']\n",
    "# Number of features to consider at every split\n",
    "max_features = ['log2', 'sqrt',.10,.15]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [1,5,10]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "#bootstrap = [True, False]\n",
    "# Create the parameter grid\n",
    "param_grid = dict(n_estimators=n_estimators, max_features=max_features, max_depth=max_depth,criterion=criterion,\n",
    "               min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,random_state=random_state)\n",
    "\n",
    "#Run the grid search\n",
    "#import time\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "grid_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv = 5, n_jobs=-1)\n",
    "\n",
    "#start_time = time.time()\n",
    "grid_result_rfc = grid_rfc.fit(X_train[embeded_rf_feature], y_train)\n",
    "# Summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result_rfc.best_score_, grid_result_rfc.best_params_))\n",
    "\n",
    "# #The below is for execution time - not important for us right now, so commented out\n",
    "# # print(\"Execution time: \" + str((time.time() - start_time)) + ' ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign selected hyperperameters to a variable for ensamble ranking during the evaluation stage\n",
    "rfc_Peram_dict = grid_result_rfc.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify/Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the model using the test sets\n",
    "y_RF_pred = grid_rfc.predict(X_test[embeded_rf_feature])\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_RF_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of the Classification Report\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_test, y_RF_pred))\n",
    "\n",
    "#AUC for y\n",
    "y_RF_pred_proba = grid_rfc.predict_proba(X_test[embeded_rf_feature])[::,1]\n",
    "fpr_RF, tpr_RF, _ = metrics.roc_curve(y_test,  y_RF_pred_proba)\n",
    "auc_RF = metrics.roc_auc_score(y_test, y_RF_pred_proba)\n",
    "plt.plot(fpr_RF,tpr_RF,label=\"auc=\"+str(auc_RF))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "print(\"\")\n",
    "\n",
    "#Print Confusion Matrix\n",
    "cnf_matrix_RF = metrics.confusion_matrix(y_test, y_RF_pred)\n",
    "cnf_matrix_RF\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_RF), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix RF', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Support Vector Machines (SVM) Classification Model (SVC)\n",
    ">   <b> Accuracy: </b> 88%\n",
    "<br><b> AUC: </b> 85%\n",
    "<br><b> Next Steps: </b> SVC ensemble classifiers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###1. Step 1: Specify the Model\n",
    "from sklearn import svm\n",
    "svc = svm.SVC(kernel='rbf',cache_size=7000,gamma= 'scale', C=1, probability =True, degree = 3) # gamma= 0.001 , kernel='poly', 'rbf',‘linear’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection: SelectFromModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "svc = svm.SVC(kernel='linear',cache_size=7000,gamma= 'auto', C=5, probability =True, degree = 5)\n",
    "\n",
    "svc_selector = RFE(estimator=svc, n_features_to_select=14, step=20, verbose=5)\n",
    "svc_selector.fit(X_train, y_train)\n",
    "svc_support = svc_selector.get_support()\n",
    "svc_feature = X_train.loc[:,svc_support].columns.tolist()\n",
    "\n",
    "print(str(len(svc_feature)), 'selected features')\n",
    "print(svc_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparamerter Optimization: GridSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the grid\n",
    "# kernel =['linear','poly','rbf']\n",
    "# C_penalty=[1.0, .75, .5] #From tutorial - C is for how much to \"penalize\" classifier; default is 1 and it can go down to 0\n",
    "# cache_size=[7000,2000,1000,500] \n",
    "# gamma= ['auto','scale', 'auto']\n",
    "# degree=[1,3,5]\n",
    "# tol=[0.003,0.005]\n",
    "param_grid =[{'C': [1, 10, 100, 1000], 'kernel': ['linear'],'cache_size': [1000,3000,5000],'probability': [True]},{'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf'],'cache_size': [1000,3000,5000],'probability': [True]}]\n",
    "\n",
    "#Run the grid search\n",
    "\n",
    "SVM = svm.SVC()\n",
    "grid_svc = GridSearchCV(estimator=SVM, param_grid=param_grid, cv = 5, n_jobs=-1)\n",
    "\n",
    "# start_time = time.time()\n",
    "grid_result_svc = grid_svc.fit(X_train[svc_feature], y_train)\n",
    "# Summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result_svc.best_score_, grid_result_svc.best_params_))\n",
    "\n",
    "#The below is for execution time - not important for us right now, so commented out\n",
    "# print(\"Execution time: \" + str((time.time() - start_time)) + ' ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign selected hyperperameters to a variable for ensamble ranking during the evaluation stage\n",
    "svc_Peram_dict = grid_result_svc.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify/Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the response for test dataset\n",
    "y_pred_svc = grid_svc.predict(X_test[svc_feature])\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy_svc:\",metrics.accuracy_score(y_test, y_pred_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create classification report\n",
    "print(classification_report(y_test, y_pred_svc))\n",
    "\n",
    "#Compute AUC\n",
    "y_SVM_pred_proba = grid_svc.predict_proba(X_test[svc_feature])[::,1]\n",
    "fpr_SVM, tpr_SVM, _ = metrics.roc_curve(y_test,  y_SVM_pred_proba)\n",
    "auc_SVM = metrics.roc_auc_score(y_test, y_SVM_pred_proba)\n",
    "plt.plot(fpr_SVM,tpr_SVM,label=\"auc=\"+str(auc_SVM))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "print(\"\")\n",
    "\n",
    "#Print Confusion Matrix/Heatmap\n",
    "cnf_matrix_svc = metrics.confusion_matrix(y_test, y_pred_svc)\n",
    "cnf_matrix_svc\n",
    "class_names_svc=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks_svc = np.arange(len(class_names_svc))\n",
    "plt.xticks(tick_marks_svc, class_names_svc)\n",
    "plt.yticks(tick_marks_svc, class_names_svc)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_svc), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix svc', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. XGBoost Model (XGB)\n",
    ">   <b> Accuracy: </b> 89%\n",
    "<br><b> AUC: </b> 89%\n",
    "<br><b> RMSE: </b> .32\n",
    "<br><b> Next Steps: </b> Additional parameter tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Needed Packages\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection: SelectFromModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://towardsdatascience.com/feature-selection-techniques-1bfab5fe0784\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "regr = LassoCV(cv=5, random_state=101,n_jobs =10 )\n",
    "regr.fit(X_train, y_train)\n",
    "print(\"LassoCV Best Alpha Scored: \", regr.alpha_)\n",
    "print(\"LassoCV Model Accuracy: \", regr.score(X_test, y_test))\n",
    "model_coef = pd.Series(regr.coef_, index = list(X.columns[:]))\n",
    "print(\"Variables Eliminated: \", str(sum(model_coef == 0)))\n",
    "print(\"Variables Kept: \", str(sum(model_coef != 0))) \n",
    "#lasso_support = regr.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate an XGBoost Classifer Model\n",
    "XGB_class = xgb.XGBClassifier(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 5, alpha = 10, n_estimators = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "figure(num=None, figsize=(12, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "top_coef = model_coef.sort_values() # possible sort by index\n",
    "top_coef[top_coef != 0].plot(kind = \"barh\")\n",
    "plt.title(\"Most Important Features Identified using Lasso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_lasso = top_coef[abs(top_coef) > 0.035]\n",
    "lasso_best = top_lasso.index.tolist()\n",
    "display(lasso_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparamerter Optimization: GridSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the grid\n",
    "objective=['reg:squarederror','binary:logistic']\n",
    "learning_rate=[.1,0.5,1]\n",
    "max_depth=[1, 3, 5]\n",
    "colsample_bytree = [0.3,0.2,0.1]\n",
    "param_grid_xgb = dict(objective=objective,learning_rate=learning_rate,max_depth=max_depth,colsample_bytree=colsample_bytree)\n",
    "\n",
    "#Run the grid search\n",
    "XGB = xgb.XGBClassifier(n_estimators = 100)\n",
    "grid_xgb = GridSearchCV(estimator=XGB, param_grid=param_grid_xgb, cv = 10, n_jobs=-1)\n",
    "\n",
    "#Train the model\n",
    "grid_result_xgb = grid_xgb.fit(X_train[lasso_best], y_train)\n",
    "\n",
    "# Summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result_xgb.best_score_, grid_result_xgb.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign selected hyperperameters to a variable for ensamble ranking during the evaluation stage\n",
    "xgb_Peram_dict = grid_result_xgb.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify/Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put Data into structure for XGBoost\n",
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_XGB_pred = grid_xgb.predict(X_test[lasso_best])\n",
    "\n",
    "print(\"y Accuracy:\",metrics.accuracy_score(y_test, y_XGB_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE\n",
    "rmse_XGB = np.sqrt(mean_squared_error(y_test, y_XGB_pred))\n",
    "print(\"XGBoost's RMSE for y2 is: %f\" % (rmse_XGB))\n",
    "\n",
    "# #Create error ratio to evaluate results for y\n",
    "# target_range_XGB = y.max() - y.min()\n",
    "# print(\"XGB target range is: %f\" % (target_range_XGB))\n",
    "# error_ratio_XGB = rmse_XGB/target_range_XGB\n",
    "# print(\"XGBoost's Error Ratio for y2 is: %f\" % (error_ratio_XGB))\n",
    "\n",
    "# Create classification report\n",
    "print(metrics.classification_report(y_test, y_XGB_pred, labels=class_names))\n",
    "\n",
    "# Compute AUC\n",
    "y_XGB_pred_proba = grid_xgb.predict_proba(X_test[lasso_best])[::,1]\n",
    "fpr_XGB, tpr_XGB, _ = metrics.roc_curve(y_test,  y_XGB_pred_proba)\n",
    "auc_XGB = metrics.roc_auc_score(y_test, y_XGB_pred_proba)\n",
    "plt.plot(fpr_XGB,tpr_XGB,label=\"auc=\"+str(auc_XGB))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "print(\"\")\n",
    "\n",
    "# Print Confusion Matrix/Heatmap\n",
    "cnf_matrix_XGB = metrics.confusion_matrix(y_test, y_XGB_pred)\n",
    "cnf_matrix_XGB\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_XGB), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix XGB', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Neural Network Model (NN)\n",
    ">   <b> Accuracy: </b> 89%\n",
    "<br><b> AUC: </b> 90%\n",
    "<br><b> Next Steps: </b> Additional parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y1_Models_train, y1_Models_test, y2_Models_train, y2_Models_test\n",
    "\n",
    "\n",
    "###1. Step 1: Specify the Model\n",
    "# Import the model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Initializing the multilayer perceptron\n",
    "# mlp = MLPClassifier(hidden_layer_sizes = (3,1),solver='sgd',learning_rate_init= 0.01, max_iter=50)\n",
    "# mlp= MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto', beta_1=0.9, \n",
    "# beta_2=0.999, early_stopping=False, epsilon=1e-08,       \n",
    "# hidden_layer_sizes=(7,3), learning_rate='adaptive',      \n",
    "# learning_rate_init=0.01, max_iter=10, momentum=0.9,       \n",
    "# nesterovs_momentum=True, power_t=0.5, random_state=1000,       \n",
    "# shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,       \n",
    "# verbose=False, warm_start=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection: SelectFromModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_feature_selection = best_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparamerter Optimization: GridSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETER OPTIMIZATION CODE USING GRID SEARCH: \n",
    "\n",
    "#Add in GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "mlp= MLPClassifier()\n",
    "\n",
    "#Define the grid\n",
    "activation=['identity','logistic', 'tanh', 'relu']\n",
    "batch_size=['auto',50,100]\n",
    "solver=['adam','lbfgs','sgd']\n",
    "param_grid = dict(activation=activation,batch_size=batch_size,solver=solver)\n",
    "\n",
    "#Run the grid search\n",
    "#import time\n",
    "\n",
    "grid_nn = GridSearchCV(estimator=mlp, param_grid=param_grid, cv = 5, n_jobs=-1)\n",
    "\n",
    "#start_time = time.time()\n",
    "grid_result_nn = grid_nn.fit(X_train[nn_feature_selection], y_train)\n",
    "# Summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result_nn.best_score_, grid_result_nn.best_params_))\n",
    "\n",
    "#The below is for execution time - not important for us right now, so commented out\n",
    "# print(\"Execution time: \" + str((time.time() - start_time)) + ' ms'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign selected hyperperameters to a variable for ensamble ranking during the evaluation stage\n",
    "NN_Peram_dict_1 = grid_result_nn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETER OPTIMIZATION CODE USING GRID SEARCH: \n",
    "\n",
    "#Add in GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "mlp2 = MLPClassifier(activation= 'logistic', batch_size= 50, solver= 'adam')\n",
    "\n",
    "\n",
    "#Define the grid\n",
    "hidden_layer_sizes2=[3,5,6,7,10,[6,2],[5,3],[4,3],[4,2],[8,3],[7,3]]\n",
    "# activation2=['tanh']\n",
    "# batch_size2=[50]\n",
    "learning_rate2=['adaptive','constant','invscaling']      \n",
    "random_state2=[100,500,1000,2000,3000]       \n",
    "# solver2=['adam']\n",
    "\n",
    "param_grid2 = dict(hidden_layer_sizes=hidden_layer_sizes2,activation=['logistic'],batch_size=[50],learning_rate=learning_rate2,random_state=random_state2,solver=['adam'])\n",
    "\n",
    "#Run the grid search\n",
    "#import time\n",
    "\n",
    "grid_nn2 = GridSearchCV(estimator=mlp2, param_grid=param_grid2, cv = 3, n_jobs=-1)\n",
    "\n",
    "#start_time = time.time()\n",
    "grid_result_nn2 = grid_nn2.fit(X_train[nn_feature_selection], y_train)\n",
    "# Summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result_nn2.best_score_, grid_result_nn2.best_params_))\n",
    "\n",
    "#The below is for execution time - not important for us right now, so commented out\n",
    "# print(\"Execution time: \" + str((time.time() - start_time)) + ' ms'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign selected hyperperameters to a variable for ensamble ranking during the evaluation stage\n",
    "NN_Peram_dict_2 = grid_result_nn2.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify/Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict response for test data\n",
    "y_pred_nn = grid_nn2.predict(X_test[nn_feature_selection])\n",
    "\n",
    "# Print model accuracy (i.e., how often the classifier is correct)\n",
    "print(\"MLP score is\",grid_nn2.score(X_test[nn_feature_selection],y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification report\n",
    "print(classification_report(y_test, y_pred_nn))\n",
    "\n",
    "# Compute AUC\n",
    "y_pred_proba_nn = grid_nn2.predict_proba(X_test[nn_feature_selection])[::,1]\n",
    "fpr_nn, tpr_nn, _ = metrics.roc_curve(y_test,  y_pred_proba_nn)\n",
    "auc_nn = metrics.roc_auc_score(y_test, y_pred_proba_nn)\n",
    "plt.plot(fpr_nn,tpr_nn,label=\"data 1, auc=\"+str(auc_nn))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "\n",
    "# Print Confusion Matrix/Heatmap\n",
    "cnf_matrix_nn = metrics.confusion_matrix(y_test, y_pred_nn)\n",
    "cnf_matrix_nn\n",
    "class_names_nn=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks_nn = np.arange(len(class_names_nn))\n",
    "plt.xticks(tick_marks_nn, class_names_nn)\n",
    "plt.yticks(tick_marks_nn, class_names_nn)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_nn), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix NN', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">   <b> Accuracy: </b> 89%\n",
    "<br><b> AUC: </b> 87%\n",
    "<br><b> Next Steps: </b> Additional parameter tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the model\n",
    "# import the class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate the model (using the default parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection: SelectFromModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded_lr_selector = SelectFromModel(LogisticRegression(penalty=\"l2\"), max_features=10)\n",
    "embeded_lr_selector.fit(X_train, y_train)\n",
    "\n",
    "embeded_lr_support = embeded_lr_selector.get_support()\n",
    "embeded_lr_feature = X.loc[:,embeded_lr_support].columns.tolist()\n",
    "print(str(len(embeded_lr_feature)), 'selected features')\n",
    "print(embeded_lr_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparamerter Optimization: GridSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETER OPTIMIZATION CODE USING GRID SEARCH: \n",
    "\n",
    "#Add in GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Define the grid\n",
    "dual=[True,False]\n",
    "max_iter=[100,110,120,130,140]\n",
    "C=[1.0,1.5,2.0,2.5]\n",
    "solver=['newton-cg','lbfgs', 'liblinear', 'sag', 'saga']\n",
    "penalty=['l1', 'l2', 'elasticnet', 'none']\n",
    "param_grid = dict(dual=dual,max_iter=max_iter,C=C,solver=solver,penalty=penalty)\n",
    "\n",
    "#Run the grid search\n",
    "lr = LogisticRegression()\n",
    "grid_lr = GridSearchCV(estimator=lr, param_grid=param_grid, cv = 10, n_jobs=-1, scoring='f1')\n",
    "\n",
    "#start_time = time.time()\n",
    "grid_result_lr = grid_lr.fit(X_train[embeded_lr_feature], y_train)\n",
    "# Summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result_lr.best_score_, grid_result_lr.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign selected hyperperameters to a variable for ensamble ranking during the evaluation stage\n",
    "lr_Peram_dict = grid_result_lr.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify/Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict response for test data\n",
    "y_LR_pred=grid_lr.predict(X_test[embeded_lr_feature])\n",
    "\n",
    "# Print model accuracy (i.e., how often the classifier is correct)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_LR_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Classification Report\n",
    "print(metrics.classification_report(y_test, y_LR_pred))\n",
    "\n",
    "# Compute AUC\n",
    "y_LR_pred_proba = grid_lr.predict_proba(X_test[embeded_lr_feature])[::,1]\n",
    "fpr_LR, tpr_LR, _ = metrics.roc_curve(y_test,  y_LR_pred_proba)\n",
    "auc_LR = metrics.roc_auc_score(y_test, y_LR_pred_proba)\n",
    "plt.plot(fpr_LR,tpr_LR,label=\"data 1, auc=\"+str(auc_LR))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "\n",
    "# Print the Confusion Matrix/Heatmap\n",
    "cnf_matrix_LogR = metrics.confusion_matrix(y_test, y_LR_pred)\n",
    "cnf_matrix_LogR\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_LogR), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix Log Reg', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluate all the methods and compare using ensemble ranking  <a name=\"part4.4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_Optimized = DecisionTreeClassifier(**dt_Peram_dict)\n",
    "RF_Optimized = RandomForestClassifier(**rfc_Peram_dict)\n",
    "SVM_Optimized = svm.SVC(**svc_Peram_dict)\n",
    "XGB_Optimized = xgb.XGBClassifier(**xgb_Peram_dict)\n",
    "GNB_Optimized = GaussianNB()\n",
    "LR_Optimized = LogisticRegression(**lr_Peram_dict)\n",
    "NN_Optimized = MLPClassifier(**NN_Peram_dict_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare standalone models for binary classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from matplotlib import pyplot\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    models['lr'] = LR_Optimized\n",
    "    models['xgb'] = XGB_Optimized\n",
    "    models['tree'] = DT_Optimized\n",
    "    models['svm'] = SVM_Optimized\n",
    "    models['nn'] = NN_Optimized\n",
    "    models['bayes'] = GNB_Optimized\n",
    "    models['rforest'] = RF_Optimized\n",
    "    return models\n",
    "\n",
    "# evaluate a given model using cross-validation\n",
    "def evaluate_model(model):\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=500)\n",
    "    scores = cross_val_score(model, X, y, scoring='f1', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    return scores\n",
    "\n",
    "# define dataset\n",
    "#X, y = X_test,y_test\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "    scores = evaluate_model(model)\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a stacking ensemble of models\n",
    "def get_stacking():\n",
    "# define the base models\n",
    "    level0 = list()\n",
    "    level0.append(('xgb', XGB_Optimized))\n",
    "    level0.append(('tree', DT_Optimized))\n",
    "    level0.append(('forest',RF_Optimized ))\n",
    "    level0.append(('lr', LR_Optimized))\n",
    "    level0.append(('svm', SVM_Optimized))\n",
    "    level0.append(('nn', NN_Optimized))\n",
    "    level0.append(('bayes', GNB_Optimized))\n",
    "# define meta learner model\n",
    "    level1 = XGB_Optimized\n",
    "# define the stacking ensemble\n",
    "    model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_stacked_models():\n",
    "    models = dict()\n",
    "    models['lr'] = LR_Optimized\n",
    "    models['xgb'] = XGB_Optimized\n",
    "    models['tree'] = DT_Optimized\n",
    "    models['svm'] = SVM_Optimized\n",
    "    models['nn'] = NN_Optimized\n",
    "    models['bayes'] = GNB_Optimized\n",
    "    models['rforest'] = RF_Optimized\n",
    "    models['stacking'] = get_stacking()\n",
    "    return models\n",
    "\n",
    "\n",
    "# # evaluate stacked given model using cross-validation\n",
    "# def evaluate_stacked_model(model):\n",
    "# \tcv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=1)\n",
    "# \tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "# \treturn scores\n",
    "\n",
    "# define dataset\n",
    "X, y = X_test,y_test\n",
    "# get the models to evaluate\n",
    "models_stacked = get_stacked_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models_stacked.items():\n",
    "    scores = evaluate_model(model)\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Run and evaluate the final models using TPOT  <a name=\"part4.6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "\n",
    "# from tpot import TPOTRegressor # for regression tasks\n",
    "\n",
    "tpot = TPOTClassifier(generations=5,verbosity=2, n_jobs=-1,max_time_mins=45, \n",
    "                      max_eval_time_mins=5, scoring='f1_weighted',verbosity=2)\n",
    "\n",
    "tpot.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
