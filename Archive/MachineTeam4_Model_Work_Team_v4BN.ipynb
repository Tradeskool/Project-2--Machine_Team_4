{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Team 4 (Michael DiSanto, Dawn Massey & Brian Nicholls)\n",
    "### BA545: Data Mining - Competition #2 (Online Shoppers' Purchasing Intentions)\n",
    "#### Data Audit Report - Spring 2020\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.ytimg.com/vi/CRKn-9gVNBw/maxresdefault.jpg\" width=60%/>\n",
    "\n",
    "Note: This work was completed using the CRISP-DM Framework shown above; accordingly, it will serve as an organizing framework for this report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Table of Contents:**\n",
    "\n",
    "0. [Part 0: Preparing for Analysis](#part0)\n",
    "1. [Part I: Business Issue Understanding](#part1)\n",
    "2. [Part II: Data Understanding & Exploratory Data Analysis (EDA)](#part2)\n",
    "3. [Part III: Data Preparation](#part3)\n",
    "\n",
    "#### **Note: Parts V and onward are for future work**\n",
    "4. [Part IV: Data Analysis/Modeling](#part4)\n",
    "5. [Part V: Validation](#part5)\n",
    "6. [Part VI: Presentation/Visualization](#part6)\n",
    "7. [Part VII: Sources](#part7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Preparing for Analysis  <a name=\"part0\"></a>\n",
    "#### Import the necesary packages for reading, analyzing, tidying, medeling, & evaluating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO USE FOR ENTIRE TEAM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas_profiling import ProfileReport\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO USE FOR DWM ONLY\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from pandas_profiling import ProfileReport\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the data\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "# scaler = StandardScaler().fit(X_train) >>> standardized_X = scaler.transform(X_train) >>> standardized_X_test = scaler.transform(X_test\n",
    "from sklearn.preprocessing import Normalizer\n",
    "# scaler = Normalizer().fit(X_train) >>> normalized_X = scaler.transform(X_train) >>> normalized_X_test = scaler.transform(X_test)\n",
    "from sklearn.preprocessing import Binarizer \n",
    "# binarizer = Binarizer(threshold=0.0).fit(X) >>> binary_X = binarizer.transform(X)\n",
    "\n",
    "# Encoding Categorical Features\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# enc = LabelEncoder()\n",
    "# y = enc.fit_transform(y)\n",
    "from sklearn.impute import (SimpleImputer, KNNImputer, MissingIndicator)\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "# imp = Imputer(missing_values=0, strategy='mean', axis=0) >>> imp.fit_transform(X_train)\n",
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "# poly = PolynomialFeatures(5) >>> poly.fit_transform(X)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=0)\n",
    "\n",
    "\n",
    "# Various Models\n",
    "from sklearn.cluster import KMeans\n",
    "# k_means = KMeans(n_clusters=3, random_state=0\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=0.95)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# logreg = LogisticRegression()\n",
    "from sklearn.linear_model import RidgeCV\n",
    "# rrm = RidgeCV(alphas=(0.01, 0.1, 1.0, 10.0), normalize=True)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# gnb = GaussianNB()\n",
    "from sklearn.svm import SVC \n",
    "# svc = SVC(kernel='linear')\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# lr = LinearRegression(normalize=True)\n",
    "from sklearn import neighbors\n",
    "# knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "## Fit the model\n",
    "# # Supervised learning\n",
    "# lr.fit(X, y)\n",
    "# knn.fit(X_train, y_train)\n",
    "# svc.fit(X_train, y_train)   \n",
    "\n",
    "# #Unsupervised Learning \n",
    "# k_means.fit(X_train) \n",
    "# pca_model = pca.fit_transform(X_train)\n",
    "\n",
    "## Predict Y\n",
    "# Supervised Estimators\n",
    "# y_pred = svc.predict(np.random.random((2,5))) \n",
    "# y_pred = lr.predict(X_test)\n",
    "# y_pred = knn.predict_proba(X_test)   \n",
    "# Unsupervised Estimators \n",
    "# y_pred = k_means.predict(X_test)\n",
    "\n",
    "\n",
    "# Packages to evaluate Model Performance (Classification)\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report \n",
    "# print(classification_report(y_test_log, y_pred_log))\n",
    "\n",
    "# Packages to evaluate Model Performance (Linear)\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "# y_true = [3, -0.5, 2] >>> mean_absolute_error(y_true, y_pred)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# mean_squared_error(y_test, y_pred)\n",
    "from sklearn.metrics import r2_score \n",
    "# r2_score(y_true, y_pred)\n",
    "\n",
    "#from sklearn.cross_validation import cross_val_score \n",
    "# print(cross_val_score(knn, X_train, y_train, cv=4)) >>> print(cross_val_score(lr, X, y, cv=2)\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in our original data\n",
    "df = pd.read_csv('online_shoppers_intention.csv', na_values=r'-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Business Issue Understanding  <a name=\"part1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Research Question:\n",
    "Overall, this project's research question is: *What drives potential customers to make purchases?*\n",
    "\n",
    "## B. Scope of Work:\n",
    "This project is a classification project in which the members of Machine Team 4 (Michael DiSanto, Dawn Massey and Brian Nicholls) will use the data feature, Revenue, as the target feature when predicing whether a consumer made a purchase and, thus, is part of Class 1 (i.e., if Revenue > 0) or, instead, whether the consumer did not make a purchase and, thus, is part of Class 0 (i.e., if Revenue <= 0). \n",
    "\n",
    "Using the 10 numerical (continous) and 8 categorical features in the given dataset, members of the team we will utilize advanced and novel methods in preparing the data to design and implement a model for the client that will predict whether a site visitors will make a purchase. The model will be evaluated on the basis of its prediction accuracy and its predictive power. \n",
    "\n",
    "Deliverable due dates are as follows:\n",
    "*     Data Audit Report due Tuesday, March 31, 2020\n",
    "*     Initial Data Model due Tuesday, April 14, 2020\n",
    "*     Final Presentation and Report due Tuesday, April 28, 2020\n",
    "\n",
    "## C. Business Understanding:\n",
    "Online shopping is an important revenue source for many retail businesses, such as our client. According to Sakar et al. (2019), desipte increases in e-commerce traffic in the recent past, \"conversion\" (of browsers to purchasers) has not increased proportionately. Indeed, the dataset includes 12,330 \"sessions,\" of which only 1908 (15.5%) resulted in conversion (Sakar et al. 2019, 6895). Thus, it is very important for retail companies, such as our client, to better understand - in real time - the cues that drive conversion. Complicating the process is that unlike \"brick and mortar\" stores where shoppers can interact with salespeople who, in turn, can help to facilitate (or at least understand) customer conversion on the basis of their interaction, online retail businesses must *infer* customer behavior from other cues. But, what are those cues for our client? As part of determining whether a \"browser\" will become a \"purchaser,\" our client also might like to know about the cues suggesting the opposite behavior - i.e., abandoning the site or the shopping cart. Additionally, the client might like to know why customers purchasing competitors' products fail to visit their website. On the basis of our models, our client is interested in knowing what we might suggest doing (in real time) to increase conversion/reduce abandonment. Further our client might also like to know if we believe there are other factors that have not been captured in the dataset that might be helpful in better predicting conversion/abandonment in the future as well as figuring out how to attract to the client's websitem consumers who make purchases on competitors' websites.\n",
    "\n",
    "Past research has focused on: \n",
    "clickstream data\n",
    "session information\n",
    "session length in terms of the number of Web pages visited in a session\n",
    "session duration in seconds\n",
    "average time per page in seconds\n",
    "traffic type (representing the page that referred the user to a particular (bookstore) site\n",
    "three binary variables representing a set of key perations related to the commercial intent\n",
    "a set of product categories viewed during the session \n",
    "sequential data/most frequently followed navigation paths\n",
    "(see Sakar et al. 2019, p. 6894, ff.)\n",
    "\n",
    "\n",
    "### _Reference:_\n",
    "Sakar, C., S. Polat, M. Katircioglu and Y. Kastro. 2019. Real-time predicgion of online shoppers' purchasing intention using multilayer perceptron and LSTM recurrent neural networks. *Neural Computing and Applications 31:* 6893-6908.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Data Understanding & Exploratory Data Analysis  <a name=\"part2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Data Understanding\n",
    "Data Understanding includes providing an overview of the dataset, conducting exploratory data analysis, verifying data quality, and deciding how to address data quality issues.\n",
    "\n",
    "#### _1. Overview of Dataset_\n",
    "The dataset that has been gathered for purposes of this analysis contains 18 variables: Revenue, which is the Target Variable (where Revenue = TRUE if the customer visiting the website made a purchase - i.e., Class 1; and Revenue = FALSE if the customer visiting the website did not make a purchase - i.e., Class 0); and 17 predictor variables, including 10 continuous features and 7 categorical features, each of which are listed below and then delineated within our Data Dictionary.\n",
    "\n",
    "\n",
    "##### a. Continuous Features:\n",
    "*     Administrative: Number of pages visited by the visitor about account management  \n",
    "*     Administrative Duration: The total amount of time (in seconds) the visitor spent on account management-related pages\n",
    "*     Informational: Number of pages visited by the visitor about Web site and its communciation and address information \n",
    "*     Informational Duration: The total amount of time (in seconds) the visitor spent on informational pages\n",
    "*     Product Related: Number of pages visited by the visitor about product-related pages  \n",
    "*     Product Related Duration: The total amount of time (in seconds) the visitor spent on product-related pages  \n",
    "*     Bounce Rate: Average bounce rate value of the pages visited by the visitor\n",
    "*          (Note: a \"bounce\" occurs when a visitor enters the site from a particular page and then leaves the site (bounces) \n",
    "*          without any further activity.)\n",
    "*     Exit Rate: Average exit rate value of the pages visited by the visitor\n",
    "*     Page Value: Average page value of the pages visited by the visitor\n",
    "*     Special Day: Closeness of the visitor's visit to the site to a special day (e.g., Mother's Day, Valentine's Day)\n",
    "\n",
    "\n",
    "##### b. Categorical Features:\n",
    "*     OperatingSystems: Operating system of the visitor (8 possible operating systems)\n",
    "*     Browser: Browser of the visitor (13 possible browsers)\n",
    "*     Region: Geographic region from which the sesion has been started by the visitor (9 possible regions)\n",
    "*     TrafficType: Traffic source by which the visitor has arrived at the Web site - e.g., banner, SMS, direct (20 possible types)\n",
    "*     VisitorType: Visitor type as \"New Visitor,\" \"Returning Visitor,\" and \"Other\" (3 possible types)\n",
    "*     Weekend: Boolean value indicating whether the date of the visit is a weekend (2 possible values)\n",
    "*     Month: Month value for visit date (12 possible months)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c. Data Dictionary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"tg\">\n",
    "<tbody>\n",
    "</tr>\n",
    "<tr>\n",
    "<td class=\"tg-7btt\" style=\"text-align: center;\" colspan=\"4\"><strong>Data Dictionary</strong></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<tr>\n",
    "<th class=\"tg-0pky\">Variable</th>\n",
    "<th class=\"tg-0pky\">Variable Name</th>\n",
    "<th class=\"tg-0pky\">Variable Definition</th>\n",
    "<th class=\"tg-fymr\">Data Type</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td class=\"tg-7btt\" style=\"text-align: center;\" colspan=\"4\"><strong>Web Page Analytics &ndash; Numerical</strong></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td class=\"tg-0pky\"><strong>Home Page</strong></td>\n",
    "<td class=\"tg-fymr\">Administrative</td>\n",
    "<td class=\"tg-fymr\">Number of pages visited by the visitor about account management.</td>\n",
    "<td class=\"tg-0pky\">Continuous/Float</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td class=\"tg-0pky\"><strong>Home Page Duration</strong></td>\n",
    "<td class=\"tg-fymr\">Administrative_Duration</td>\n",
    "<td class=\"tg-fymr\">The total amount of time (in seconds) the visitor spent on account management-related pages.</td>\n",
    "<td class=\"tg-0pky\">Continuous/Float</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td class=\"tg-0pky\"><strong>Inforation Page</strong></td>\n",
    "<td class=\"tg-fymr\">Informational</td>\n",
    "<td class=\"tg-fymr\">Number of pages visited by the visitor about Web site and its communciation and address information.</td>\n",
    "<td class=\"tg-0pky\">Continuous/Float</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td class=\"tg-0pky\"><strong>Informational Duration</strong></td>\n",
    "<td class=\"tg-fymr\">Informational Duration</td>\n",
    "<td class=\"tg-fymr\">The total amount of time (in seconds) the visitor spent on informational pages.</td>\n",
    "<td class=\"tg-0pky\">Continuous/Float</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td class=\"tg-0pky\"><strong>Product Page</strong></td>\n",
    "<td class=\"tg-fymr\">ProductRelated</td>\n",
    "<td class=\"tg-fymr\">Number of pages visited by the visitor about product-related pages.</td>\n",
    "<td class=\"tg-0pky\">Continuous/Float</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td class=\"tg-0pky\"><strong>Product Related Duration</strong></td>\n",
    "<td class=\"tg-fymr\">ProductRelated_Duration</td>\n",
    "<td class=\"tg-fymr\">The total amount of time (in seconds) the visitor spent on product-related pages</td>\n",
    "<td class=\"tg-0pky\">Continuous/Float</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td class=\"tg-0pky\"><strong>Bounce Rate</strong></td>\n",
    "<td class=\"tg-fymr\">BounceRates</td>\n",
    "<td class=\"tg-fymr\">The percentage of single page visits (or web sessions). It is the percentage of visits in which a person leaves your website from the landing page without browsing any further.</td>\n",
    "<td class=\"tg-0pky\">Continuous/Float</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td class=\"tg-0pky\"><strong>Exit Rate</strong></td>\n",
    "<td class=\"tg-fymr\">ExitRates</td>\n",
    "<td class=\"tg-fymr\">For all pageviews to the page, Exit Rate is the percentage that were the last in the session</td>\n",
    "<td class=\"tg-0pky\">Continuous/Float</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td class=\"tg-0pky\"><strong>Page Value</strong></td>\n",
    "<td class=\"tg-fymr\">PageValues</td>\n",
    "<td class=\"tg-fymr\">the average value for a page that a user visited before landing on the goal page or completing an Ecommerce transaction (or both). This value is intended to give you an idea of which page in your site contributed more to your site's revenue.</td>\n",
    "<td class=\"tg-0pky\">Continuous/Float</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td class=\"tg-0pky\"><strong>Special Day</strong></td>\n",
    "<td class=\"tg-fymr\">SpecialDay</td>\n",
    "<td class=\"tg-fymr\">the closeness of the site visiting time to a specific special day (e.g. Mother’s Day, Valentine's Day) in which the sessions are more likely to be finalized with transaction.</td>\n",
    "<td class=\"tg-0pky\">Continuous/Float</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td class=\"tg-7btt\" style=\"text-align: center;\" colspan=\"4\"><strong>Web Page Analytics &ndash;Categorical</strong></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td class=\"tg-0pky\"><strong>Month</strong></td>\n",
    "<td class=\"tg-fymr\">Month</td>\n",
    "<td class=\"tg-fymr\">Month in which the visit took place</td>\n",
    "<td class=\"tg-0pky\">Categorical/Int</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td class=\"tg-0pky\"><strong>OperatingSystems</strong></td>\n",
    "<td class=\"tg-fymr\">OperatingSystems</td>\n",
    "<td class=\"tg-fymr\">Operating system of the computer in which the user used while viewing the site</td>\n",
    "<td class=\"tg-0pky\">Categorical/Int</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td class=\"tg-0pky\"><strong>Browser</strong></td>\n",
    "<td class=\"tg-fymr\">Browser</td>\n",
    "<td class=\"tg-fymr\">Browser in which the user used to view the site</td>\n",
    "<td class=\"tg-fymr\">Categorical/Int</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td class=\"tg-0pky\"><strong>Region</strong></td>\n",
    "<td class=\"tg-fymr\">Region</td>\n",
    "<td class=\"tg-fymr\">Region wher ethe user is located</td>\n",
    "<td class=\"tg-fymr\">Categorical/Int</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td class=\"tg-0pky\"><strong>TrafficType</strong></td>\n",
    "<td class=\"tg-fymr\">TrafficType</td>\n",
    "<td class=\"tg-fymr\">Traffic source by which the visitor has arrived at the Web site - e.g., banner, SMS, direct (20 possible types)</td>\n",
    "<td class=\"tg-fymr\">Categorical/Int</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td class=\"tg-0pky\"><strong>Visitor Type</strong></td>\n",
    "<td class=\"tg-fymr\">VisitorType</td>\n",
    "<td class=\"tg-fymr\">Is this a returing visitor or a new visitor</td>\n",
    "<td class=\"tg-fymr\">Binary/Boolean</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td class=\"tg-0pky\"><strong>Weekend</strong></td>\n",
    "<td class=\"tg-fymr\">Weekend</td>\n",
    "<td class=\"tg-fymr\">Did the visit happen on the weekend?</td>\n",
    "<td class=\"tg-fymr\">Binary/Boolean</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td class=\"tg-0pky\"><strong>Revenue</strong></td>\n",
    "<td class=\"tg-fymr\">Revenue</td>\n",
    "<td class=\"tg-fymr\">Did the visit result in Revenue?</td>\n",
    "<td class=\"tg-fymr\">Binary/Boolean</td>\n",
    "</tr>\n",
    "<tr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### B. Exploratory Data Analysis (EDA) &  Data Quality Verification (DQV)\n",
    "\n",
    "\n",
    "#### _1. Overview of Findings from EDA & DQV (per the below):_\n",
    "*     There are 12,330 observations with one target value and 17 features.\n",
    "*     There are no missing values; however, we did note the following:\n",
    "    ** There are no observations for January and April - which suggests the dataset does not include a full year's-worth of information, which may limit our ability to assess monthly trends/differences.\n",
    "    ** A few (85) observations were coded as \"other\" - meaning they were neither new nor returning customers; since \"new\" and \"returning\" customers are mutually exclusive, the observations coded as \"other\" appear to be erroneous.\n",
    "    ** 85% of the data come from Browser 1 (20%) or 2 (65%); hence, the data are not balanced with regard to browser\n",
    "    ** Approximately 90% of the data comes from days other than \"Special\" days\n",
    "    ** \n",
    "*     Bounce Rate and Exit Rate are highly correlated at 0.91; however, they also are highly correlated with the target variable (at -0.15 for Bounce Rate and at -0.25 for Exit Rate); hence, we are reluctant to remove either from our analysis. Rather, we will consider engineering a new feature that combines Bounce Rate with Exit Rate (e.g., via a linear combination of an average or weighted average of the features).\n",
    "*     Administrative Page and Exit Rates are also highly correlated at -0.43; however, they, too, are highly correlated with the target variable (at 0.62 for Adminstative Page and at -0.25 for Exit Rate); hence, we are reluctant to remove either from our analysis. Rather, we will consider engineering a new feature that combines Administative Page with Exit Rate (e.g., via a division of one feature by the other).\n",
    "*     Our data is imbalanced toward Revenue = False (i.e, Class 0, no purchases).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _2. Descriptive Statistics:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample =df.sample(30)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "----------------------------------------------------------------------\n",
    "##### Initial import seems to accurate and complete in comparison to the data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More info on the dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "----------------------------------------------------------------------\n",
    "##### There are no null columns on import and 12330 rows and 18 columns\n",
    "##### Month, VisitorType, Weekend, and Revenue are non-numberic attributes that may need adjustment later in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inital description of the data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "##### BounceRate, ExitRate, & SpecialDay are on a 0-1 scale, while the others nummerical attributies are on a differnt scale.\n",
    "##### SpecialDay, OperatingSystem, Browser, Region, TrafficType are all categorical attributes which could be futher analyzed using encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _3. Correlation Analysis:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Heatmap for the dataframe\n",
    "spearman =df.corr(method ='spearman')\n",
    "plt.figure(figsize=(25,10))\n",
    "sns.heatmap(spearman, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_list=['Administrative','Administrative_Duration','Informational','Informational_Duration','ProductRelated','ProductRelated_Duration','BounceRates','ExitRates','PageValues','SpecialDay','OperatingSystems','Browser','Region','TrafficType']\n",
    "sns.pairplot(df[numerical_list],corner=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "##### We observed the following high correlations:\n",
    "    - The duration attributes may need to be assesed for elimination in the final model as each is very highly correlated with its corresponding non-duration attribute.\n",
    "    \n",
    "    - Administrative & ProductRelated have a correlation of 0.46; this merits further investigation and possible feature engineering.\n",
    "    - Administrative & ProductRelated_Duration have a correlation of 0.42; this merits further investigation and possible feature engineering.\n",
    "    - Administrative & ExitRates have a correlation of -0.43; this merits further investigation and possible feature engineering.\n",
    "    - Administrative & PageValues have a correlation of 0.33; this merits further investigation and possible feature engineering.\n",
    "\n",
    " \n",
    "    - Administrative_Duration & ExitRates have a correlation of -0.44; this merits further investigation and possible feature engineering.\n",
    "    - Administrative_Duration & ProductRelated have a correlation of 0.43; this merits further investigation and possible feature engineering.\n",
    "    - Administrative_Duration & ProductRelated_Duration have a correlation of 0.41; this merits further investigation and possible feature engineering. \n",
    "\n",
    "    - ProductRelated & ExitRates have a correlation of -0.52; this merits further investigation and possible feature engineering.\n",
    " \n",
    "    - ProductRelated_Duration & ExitRates have a correlation of -0.48; this merits further investigation and possible feature engineering.\n",
    "    \n",
    "    - BounceRate & ExitRates have a correlation of 0.6; this merits further investigation and possible feature engineering.\n",
    "\n",
    "##### After initial exploration we decided to compare features that are highly correlated with the target (Revenue)      \n",
    "    - PageValues & Revenue have a correlation of 0.63; As Revenue is the target we would expect PageValues to be a useful attribute in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for Images using Ploylt\n",
    "\n",
    "# import plotly.express as px\n",
    "# import os\n",
    "# from IPython.display import Image\n",
    "\n",
    "# # Admin vs PageValues Scatter plot\n",
    "# fig6 = px.scatter(df, x=\"ProductRelated\", y=\"Administrative\",facet_col=\"Revenue\", color=\"Region\", trendline=\"ols\",render_mode = 'webgl' )\n",
    "# img_bytes_6 = fig6.to_image(format=\"png\", width=1200, height=400, scale=1)\n",
    "# fig6.write_image(\"images/fig6.png\")\n",
    "# #Image(img_bytes_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/fig6.png\"/>\n",
    "\n",
    "Admin vs ProductRelated seems to not be useful features for feature engineering as the trends do not differ when segmented by Revenue True or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Code for Images using Ploylt\n",
    "\n",
    "# import plotly.express as px\n",
    "# import os\n",
    "# from IPython.display import Image\n",
    "\n",
    "# # Admin vs PageValues Scatter plot\n",
    "# fig7 = px.scatter(df, x=\"ProductRelated_Duration\", y=\"Administrative\",facet_col=\"Revenue\", color=\"Region\", trendline=\"ols\",render_mode = 'webgl' )\n",
    "# img_bytes_7 = fig7.to_image(format=\"png\", width=1200, height=400, scale=1)\n",
    "# fig7.write_image(\"images/fig7.png\")\n",
    "# #Image(img_bytes_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/fig7.png\"/>\n",
    "\n",
    "Admin vs ProductRelated_Duration seems to not be useful features for feature engineering as the trends do not differ when segmented by Revenue True or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code for Images using Ploylt\n",
    "\n",
    "# # Admin vs BounceRates Scatter plot\n",
    "# fig5 = px.scatter(df, x=\"ExitRates\", y=\"Administrative\",facet_col=\"Revenue\", color=\"Region\", trendline=\"ols\",render_mode = 'webgl' )\n",
    "# img_bytes_5 = fig5.to_image(format=\"png\", width=1200, height=400, scale=1)\n",
    "# fig5.write_image(\"images/fig5.png\")\n",
    "# # Image(img_bytes_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/fig5.png\"/>\n",
    "\n",
    "Admin vs ExitRates doesn not seem to be useful features for feature engineering as the trends do not differ when segmented by Revenue True or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code for Images using Ploylt\n",
    "\n",
    "# # Admin vs BounceRates Scatter plot\n",
    "# fig8 = px.scatter(df, x=\"PageValues\", y=\"Administrative\",facet_col=\"Revenue\", color=\"Region\", trendline=\"ols\",render_mode = 'webgl' )\n",
    "# img_bytes_8 = fig8.to_image(format=\"png\", width=1200, height=400, scale=1)\n",
    "# fig8.write_image(\"images/fig8.png\")\n",
    "# # Image(img_bytes_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/fig8.png\"/>\n",
    "\n",
    "Admin vs Pages values seems to be useful features for feature engineering as the trends differ when segmented by Revenue True or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code for Images using Ploylt\n",
    "\n",
    "# # Admin vs BounceRates Scatter plot\n",
    "# fig9 = px.scatter(df, x=\"ExitRates\", y=\"Administrative_Duration\",facet_col=\"Revenue\", color=\"Region\", trendline=\"ols\",render_mode = 'webgl' )\n",
    "# img_bytes_9 = fig9.to_image(format=\"png\", width=1200, height=400, scale=1)\n",
    "# fig9.write_image(\"images/fig9.png\")\n",
    "# # Image(img_bytes_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/fig9.png\"/>\n",
    "\n",
    "Administrative_Duration vs ExitRates seems to not be useful features for feature engineering as the trends do not differ when segmented by Revenue True or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code for Images using Ploylt\n",
    "\n",
    "# # Admin vs BounceRates Scatter plot\n",
    "# fig10 = px.scatter(df, x=\"ProductRelated\", y=\"Administrative_Duration\",facet_col=\"Revenue\", color=\"Region\", trendline=\"ols\",render_mode = 'webgl' )\n",
    "# img_bytes_10 = fig10.to_image(format=\"png\", width=1200, height=400, scale=1)\n",
    "# fig10.write_image(\"images/fig10.png\")\n",
    "# # Image(img_bytes_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/fig10.png\"/>\n",
    "\n",
    "Administrative_Duration vs ProductRelated seems to not be useful features for feature engineering as the trends do not differ when segmented by Revenue True or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code for Images using Ploylt\n",
    "\n",
    "# # Admin vs BounceRates Scatter plot\n",
    "# fig11 = px.scatter(df, x=\"ProductRelated_Duration\", y=\"Administrative_Duration\",facet_col=\"Revenue\", color=\"Region\", trendline=\"ols\",render_mode = 'webgl' )\n",
    "# img_bytes_11 = fig11.to_image(format=\"png\", width=1200, height=400, scale=1)\n",
    "# fig11.write_image(\"images/fig11.png\")\n",
    "# # Image(img_bytes_11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/fig11.png\"/>\n",
    "\n",
    "Administrative_Duration vs ProductRelated_Duration seems to not be useful features for feature engineering as the trends do not differ when segmented by Revenue True or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code for Images using Ploylt\n",
    "\n",
    "# # Admin vs BounceRates Scatter plot\n",
    "# fig13 = px.scatter(df, x=\"ExitRates\", y=\"ProductRelated\",facet_col=\"Revenue\", color=\"Region\", trendline=\"ols\",render_mode = 'webgl' )\n",
    "# img_bytes_13 = fig13.to_image(format=\"png\", width=1200, height=400, scale=1)\n",
    "# fig13.write_image(\"images/fig13.png\")\n",
    "# # Image(img_bytes_13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/fig13.png\"/>\n",
    "\n",
    "ExitRates vs ProductRelated seems to be useful features for feature engineering as the trends differ when segmented by Revenue True or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code for Images using Ploylt\n",
    "\n",
    "# # Admin vs BounceRates Scatter plot\n",
    "# fig12 = px.scatter(df, x=\"ExitRates\", y=\"ProductRelated_Duration\",facet_col=\"Revenue\", color=\"Region\", trendline=\"ols\",render_mode = 'webgl' )\n",
    "# img_bytes_12 = fig12.to_image(format=\"png\", width=1200, height=400, scale=1)\n",
    "# fig12.write_image(\"images/fig12.png\")\n",
    "# # Image(img_bytes_12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/fig12.png\"/>\n",
    "\n",
    "ExitRates vs ProductRelated_Duration seems to be useful features for feature engineering as the trends differ when segmented by Revenue True or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code for Images using Ploylt\n",
    "\n",
    "# # ExitRates vs BounceRates Scatter plot\n",
    "# fig4 = px.scatter(df, x=\"BounceRates\", y=\"ExitRates\",facet_col=\"Revenue\", color=\"Region\", trendline=\"ols\",render_mode = 'webgl' )\n",
    "# img_bytes_4 = fig4.to_image(format=\"png\", width=1200, height=400, scale=1)\n",
    "# fig4.write_image(\"images/fig4.png\")\n",
    "# Image(img_bytes_4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/fig4.png\"/>\n",
    "\n",
    "BounceRates vs ExitRates are highly correlated including when segmented by Revenue True or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code for Images using Ploylt\n",
    "\n",
    "# # BounceRates vs PageValues Scatter plot\n",
    "# fig2 = px.scatter(df, x=\"PageValues\", y=\"BounceRates\",facet_col=\"Revenue\", color=\"Region\", trendline=\"ols\",render_mode = 'webgl' )\n",
    "# img_bytes_2 = fig2.to_image(format=\"png\", width=1200, height=400, scale=1)\n",
    "# fig2.write_image(\"images/fig2.png\")\n",
    "# Image(img_bytes_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/fig2.png\"/>\n",
    "\n",
    "BounceRates vs PagesValues seems to be slightly useful features for feature engineering as the trends differ when segmented by Revenue True or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code for Images using Ploylt\n",
    "\n",
    "# # ExitRates vs PageValues Scatter plot\n",
    "# fig3 = px.scatter(df, x=\"PageValues\", y=\"ExitRates\",facet_col=\"Revenue\", color=\"Region\", trendline=\"ols\",render_mode = 'webgl' )\n",
    "# img_bytes_3 = fig3.to_image(format=\"png\", width=1200, height=400, scale=1)\n",
    "# fig3.write_image(\"images/fig3.png\")\n",
    "# Image(img_bytes_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/fig3.png\"/>\n",
    "\n",
    "ExitRates vs PagesValues seems to be slightly useful features for feature engineering as the trends differ when segmented by Revenue True or False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _4. Prelimiary EDA Visualizations:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(df)\n",
    "profile.to_file(output_file=\"Customer_Intentions_Profile.html\")\n",
    "profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations for Preliminary EDA/Visualizations:\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "##### **Warnings – highlights:**\n",
    "    -High correlation between Exit Rates and BounceRates, which we noted in the correlation map.\n",
    "    -Dataset has 125 duplicate rows, which we decided to allow given that there are no unique identifiers that we could use to verify whether the apparently duplicate entries (~1% of the data) were bogus or legitimate.\n",
    "\n",
    "##### **Variables – highlights:**\n",
    "\n",
    "    - Administrative is a count between 0 and 27; it is right skewed (with ~66% of the dataset in 0, 1, 2)\n",
    "    - Administrative_duration captures time spent; it also is right skewed with almost 50% of the data being zero (which makes sense because almost 50% of the data in Administrative is zero)\n",
    "\n",
    "    - Informational is a count between 0 and 24; it is right skewed (with ~90% of the dataset in 0, 1, 2)\n",
    "    - Informational_duration captures time spent; it also is right skewed with over 80% of the data being zero (which makes sense because over 78% of the data in Informational is zero)\n",
    "\n",
    "    - ProductRelated is a count between 0 and 705; it is right skewed (however; only ~12% of the dataset is in 0, 1, 2)\n",
    "    - ProductRelated_duration captures time spent; it also is right skewed with almost 6% of the data being zero (which makes sense because only 12% of the data in ProductRelated is zero)\n",
    "\n",
    "    - BounceRate captures the percentage of visits in which a visitor exits the landing page without browsing any further. It is right skewed, with about 45% of the data being a value of 0. Given that those who “bounce” will certainly not buy, this attribute may well be an important variable in our model. (Note: ExitRate, which is highly correlated with BounceRate, also is likely to have a similar skewness, distribution and importance in predicting online shoppers’ purchasing behavior.)\n",
    "\n",
    "    - PageValues are dollar amounts – more or less – amounting to sales amounts, divided by page views. The variable is right skewed with approximately 78% of PageValues being zero; this makes sense because about 45% of customers “bounce” immediately, buying nothing, leaving another ~30% to browse without completing a purchase.\n",
    "\n",
    "    - SpecialDay is a binary variable with 0 for not near a holiday/special day and 1 for near a holiday/special day. The variable is right skewed with approximately 90% of the data being zero (i.e., transaction not occurring near a holiday/special day). This information suggests to us that we are dealing with a unique retail environment (i.e., most retailers experience increased activity at/during holiday times).\n",
    "\n",
    "    - Month – is the month of the year in which the transaction occurred. We first noted that the dataset is devoid of transactions in January and April. Thus, the dataset does not appear to contain a full year of information, which could impair our ability to complete the analysis in light of potential seasonality. The most popular months for online browsing/shopping are: May (27.3%), November (24.3%), March (15.5%) and December 14.0%). Low months include: June-October, perhaps because           folks are not browsing/shopping online during the warmer months.\n",
    "\n",
    "    - OperatingSystems – is a categorical variable and most of the data (~95%) is in one of three operating systems (2, 1, 3). \n",
    "\n",
    "    - Browser – is a categorical variable and most the data (~91%) come from three browsers (2, 1, 4).\n",
    "\n",
    "    - Region – is a categorical variable for the region from which the visitor came. The top four account for ~77% of the data (i.e., regions 1, 3, 4, 2).\n",
    "\n",
    "    - TrafficType – is a categorical variable to indicate how the visitor arrived at the website. The top three account for approximately 67% of the referrals (i.e., types 2, 1, 3).\n",
    "\n",
    "    - VisitorType – is a categorical variable. Most visitors (~86%) are return visitors. A few visitors have been classified as “Other”; however, they should not be so classified as the categories of “Returning_visitor” and “New_visitor” should capture all visitors (i.e., a visitor is either one or the other).\n",
    "\n",
    "    - Weekend – is a categorical variable to capture whether the visitor is visiting the site on a weekend. Approximately 77% of the visits took place during the week, which makes sense in light of the proportion of weekdays in a week (i.e., 5/7 = 71.4%).\n",
    "\n",
    "    - Revenue – is the target variable. It is a categorical variable. It is imbalanced – as approximately 85% of the visits resulted in “no sale” (i.e., only ~15% of the visits resulted in sales). As such, we will need to balance the data later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Box Plots to further Describe the Data\n",
    "# For Administrative\n",
    "df_p=df.iloc[:,0]\n",
    "df_p.plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The group decided not to further consider the outliers above because the values are neither excessisvely extreme nor are they outside a reasonable range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Administrative_Duration\n",
    "df_p=df.iloc[:,1]\n",
    "df_p.plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The group decided not to further consider the outliers because there is an extrordinary amount of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Informational\n",
    "df_p=df.iloc[:,2]\n",
    "df_p.plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The group decided not to further consider the outliers because the values are neither excessisvely extreme nor are they outside a reasonable range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Informational_Duration\n",
    "df_p=df.iloc[:,3]\n",
    "df_p.plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The group decided not to further consider the outliers because there is an extrordinary amount of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ProductRelated\n",
    "df_p=df.iloc[:,4]\n",
    "df_p.plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The group decided not to further consider the outliers because there is an extrordinary amount of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ProductRelated_Duration\n",
    "df_p=df.iloc[:,5]\n",
    "df_p.plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The group decided not to further consider the outliers because there is an extrordinary amount of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For BounceRates\n",
    "df_p=df.iloc[:,6]\n",
    "df_p.plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The group decided not to further consider the outliers because there is an extrordinary amount of them and the outlier values are not outside the range of 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ExitRates\n",
    "df_p=df.iloc[:,7]\n",
    "df_p.plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The group decided not to further consider the outliers because there is an extrordinary amount of them and the outlier values are not outside the range of 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For PageValues\n",
    "df_p=df.iloc[:,8]\n",
    "df_p.plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The group decided not to further consider the outliers because there is an extrordinary amount of them\n",
    "# and even the most extreme value of $350+ is reasonable considering the definition of the attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Data Quality Improvment Strategies\n",
    "#### _1. Overview:_\n",
    "Given the noted missing/erroneous values we have formulated the following pipeline for understanding and preparing our data.\n",
    "\n",
    "We will complete the following pipeline step in the data understanding phase:\n",
    "\n",
    "        1. Imputation \n",
    "           - We will replace VisitorType \"Other\" with the mode \"Returning_Visitor\"\n",
    "\n",
    "We will complete the following pipeline steps in the data preparation phase:\n",
    "       \n",
    "       1. Feature Engineering\n",
    "           - Created new variables \n",
    "               - 'Admin_per_Exit', 'Admin_per_Bounce', 'Bounce_Exit_Rate_Avg', 'Bounce_Exit_Rate_WeightedAvg', 'Bounce_per_Exit_Rate','Total_Duration','Total_Duration_Avg','Admin_Duration_percent_TotalDuration',\\\n",
    "                  Info_Duration_percent_TotalDuration','Product_Duration_percent_TotalDuration','TotalDuration_per_PageValues', 'Admin_per_PageValues', 'AdminDuration_per_PageValues', 'Informational_per_PageValues',\\\n",
    "                  'Info_Duration_per_PageValues','ProductRelated_per_PageValues', 'Product_Duration_per_PageValues', 'Exit_per_PageValues', 'Bounce_per_PageValues'\n",
    "          \n",
    "          - Binned categorical variables to reduce the number of categories to five or fewer (Operating, Browser, Region and TrafficType,VisitorType,Month,Weekend) \n",
    "               - Binned month acorrding to Holiday month\n",
    "               - Binned month acorrding to special day frequency\n",
    "               - Binned month acorrding to target frequency\n",
    "\n",
    "       2. Outlier Detection\n",
    "           - IQR Outlier Detection: We use the IQR to address outliers in all calcualted variables\n",
    "           \n",
    "       3. Normalization\n",
    "          - utilized quantile_transform method\n",
    "          - utilized PowerTransformer as a secondary method\n",
    "       \n",
    "       4. Standardization\n",
    "           a. Min-Max Scaler\n",
    "           b. Z-Score Standardization\n",
    "              - We initially omit this step due to successfully using the Min-Max Scaler method\n",
    "           c. Standard Deviation Outlier\n",
    "              - We initially omit this step due to addressing outliers via IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline - Initial Imputation of Categorical Features:\n",
    "\n",
    "## Replace the VisitorType 'Other' with the variable's mode, namely: 'Returning_Visitor'\n",
    "df['VisitorType'] = df['VisitorType'].replace('Other','Returning_Visitor')\n",
    "df.groupby('VisitorType').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Imputing Continuous Features\n",
    "\n",
    "impute1_list =['Administrative','Administrative_Duration','Informational','Informational_Duration','ProductRelated','ProductRelated_Duration','BounceRates','ExitRates','PageValues']\n",
    "\n",
    "# Impute Zeros before doing the log\n",
    "for column in impute1_list:\n",
    "    df[column] = df[column] + 1\n",
    "\n",
    "display(df.sample(20))\n",
    "\n",
    "# df['PageValues'] = df['PageValues'] + 1\n",
    "\n",
    "df['PageValues_Log'] = np.log(df['PageValues'])\n",
    "df['PageValues_Log10'] = np.log10(df['PageValues'])\n",
    "\n",
    "df[['PageValues_Log','PageValues_Log10']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if there are any inf values or N/As\n",
    "display(df[['PageValues_Log','PageValues_Log10']].describe())\n",
    "display(df[['PageValues_Log','PageValues_Log10']].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Data Preparation  <a name=\"part3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Overview\n",
    "Data Preparation includes preprocessing steps for selecting data (e.g., including feature engineering/binning) and cleaning data (e.g., recoding for any \"new\" features created; normalizing; handling outliers; dealing with skewness; standardizing; reviewing correlations to identify highly related/correlated features that s/b avoided in the analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Selecting Data\n",
    "    a. Feature Engineering for Continuous Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship of rates or duration / page value, compare to y\n",
    "# Weighted Avg Bounce & Exit rate\n",
    "# Pipeline - Feature Engineering:\n",
    "# Created 5 new variables (Admin_per_Exit; Bounce_Exit_Rate_Avg; Bounce_per_Exit_Rate; Total_Duration; Total_Duration_Avg)\n",
    "\n",
    "\n",
    "#Create 'Admin_per_Exit' to enable us to retain two highly correlated variables (i.e., 'Administrative' and 'ExitRates') since both are highly correlated with the target\n",
    "df['Admin_per_Exit'] = df['Administrative'] / df['ExitRates']\n",
    "df['Admin_per_Bounce'] = df['Administrative'] / df['BounceRates']\n",
    "\n",
    "#Create 'Bounce_Exit_Rate_Avg' to enable us to retain two highly correlated variables (i.e., 'BounceRates' and 'ExitRates') since both are highly correlated with the target\n",
    "df['Bounce_Exit_Rate_Avg'] = (df['BounceRates'] + df['ExitRates'])/2\n",
    "df['Bounce_Exit_Rate_WeightedAvg'] = ((df['BounceRates']*.6) + (df['ExitRates']*.4))\n",
    "\n",
    "#Create 'Bounce_per_Exit_Rate' to enable us to retain two highly correlated variables (i.e., 'BounceRates' and 'ExitRates') since both are highly correlated with the target\n",
    "df['Bounce_per_Exit_Rate'] = df['BounceRates'] / df['ExitRates']\n",
    "\n",
    "\n",
    "#Create 'Total_Duration' and 'Total_Duration_Avg' to enable us to assess total and average duration, respectively.\n",
    "df['Total_Duration'] = df['Administrative_Duration'] + df['Informational_Duration'] + df['ProductRelated_Duration']\n",
    "df['Total_Duration_Avg'] = (df['Total_Duration'])/3\n",
    "df['Admin_Duration_percent_TotalDuration'] = df['Administrative_Duration'] / df['Total_Duration']\n",
    "df['Info_Duration_percent_TotalDuration'] = df['Informational_Duration'] / df['Total_Duration']\n",
    "df['Product_Duration_percent_TotalDuration'] = df['ProductRelated_Duration'] / df['Total_Duration']\n",
    "df['TotalDuration_per_PageValues'] = df['Total_Duration'] / df['PageValues']\n",
    "\n",
    "\n",
    "#Create 'Admin_per_Exit' to enable us to retain two highly correlated variables (i.e., 'Administrative' and 'ExitRates') since both are highly correlated with the target\n",
    "df['Admin_per_PageValues'] = df['Administrative'] / df['PageValues']\n",
    "df['AdminDuration_per_PageValues'] = df['Administrative_Duration'] / df['PageValues']\n",
    "\n",
    "#Create 'Admin_per_Exit' to enable us to retain two highly correlated variables (i.e., 'Administrative' and 'ExitRates') since both are highly correlated with the target\n",
    "df['Informational_per_PageValues'] = df['Informational'] / df['PageValues']\n",
    "df['Info_Duration_per_PageValues'] = df['Informational_Duration'] / df['PageValues']\n",
    "\n",
    "\n",
    "#Create 'Admin_per_Exit' to enable us to retain two highly correlated variables (i.e., 'Administrative' and 'ExitRates') since both are highly correlated with the target\n",
    "df['ProductRelated_per_PageValues'] = df['ProductRelated'] / df['PageValues']\n",
    "df['Product_Duration_per_PageValues'] = df['ProductRelated_Duration'] / df['PageValues']\n",
    "\n",
    "\n",
    "#Create 'Admin_per_Exit' to enable us to retain two highly correlated variables (i.e., 'Administrative' and 'ExitRates') since both are highly correlated with the target\n",
    "df['Exit_per_PageValues'] = df['ExitRates'] / df['PageValues']\n",
    "\n",
    "\n",
    "#Create 'Admin_per_Exit' to enable us to retain two highly correlated variables (i.e., 'Administrative' and 'ExitRates') since both are highly correlated with the target\n",
    "df['Bounce_per_PageValues'] = df['BounceRates'] / df['PageValues']\n",
    "\n",
    "\n",
    "calcualted_cols = ['Admin_per_Exit', 'Admin_per_Bounce', 'Bounce_Exit_Rate_Avg', 'Bounce_Exit_Rate_WeightedAvg', 'Bounce_per_Exit_Rate', 'Total_Duration','Total_Duration_Avg','Admin_Duration_percent_TotalDuration',\\\n",
    "           'Info_Duration_percent_TotalDuration','Product_Duration_percent_TotalDuration','TotalDuration_per_PageValues', 'Admin_per_PageValues', 'AdminDuration_per_PageValues', 'Informational_per_PageValues',\\\n",
    "           'Info_Duration_per_PageValues','ProductRelated_per_PageValues', 'Product_Duration_per_PageValues', 'Exit_per_PageValues', 'Bounce_per_PageValues','Revenue']\n",
    "\n",
    "display(df[calcualted_cols].sample(20))\n",
    "\n",
    "\n",
    "# display(df[['Administrative', 'ExitRates', 'Admin_per_Exit', 'BounceRates', 'Bounce_Exit_Rate_Avg', 'Bounce_per_Exit_Rate',\\\n",
    "#            'Administrative_Duration','Informational_Duration', 'ProductRelated_Duration', 'Total_Duration', 'Total_Duration_Avg']].sample(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check newly calcualted featues for missing values or NaN values.\n",
    "# We also analyzing the statistics for the calcualted features\n",
    "\n",
    "df[calcualted_cols].info()\n",
    "print('---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "display(df[calcualted_cols].isna().mean().round(4) * 100)\n",
    "print('---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "display(df[calcualted_cols].describe())\n",
    "print('---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "# Correlation Heatmap for the dataframe\n",
    "spearman_calculated =df.corr(method ='spearman')\n",
    "plt.figure(figsize=(35,15))\n",
    "sns.heatmap(spearman_calculated, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code below reatined in the event that it is needed in the future\n",
    "\n",
    "# # Drop calcualted columns with more than 20% missing values \n",
    "# df.drop(['Admin_per_Bounce','Admin_per_PageValuesLog','AdminDuration_per_PageValuesLog','Informational_per_PageValuesLog','Info_Duration_per_PageValuesLog','Bounce_per_PageValuesLog' ],axis=1, inplace = True)\n",
    "# # check to see if the correct column droped\n",
    "# display(df.isna().mean().round(4) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    b. Data Imputation (NO LONGER NEEDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code below reatined in the event that it is needed in the future\n",
    "\n",
    "\n",
    "# # Mean imputation\n",
    "\n",
    "# ## Impute mean values for mising values in Admin_per_Exit\n",
    "# df['Admin_per_Exit'].fillna(df['Admin_per_Exit'].mean(), inplace=True)\n",
    "\n",
    "\n",
    "# ## Impute mean values for missing values in Bounce_per_Exit_Rate\n",
    "# df['Bounce_per_Exit_Rate'].fillna(df['Bounce_per_Exit_Rate'].mean(), inplace=True)\n",
    "\n",
    "# ## Check the df\n",
    "# df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    c. Binning for Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing if there is a trend between months and Special day\n",
    "print(df.groupby('Month')['SpecialDay'].sum())\n",
    "\n",
    "# The observation is that Special days only occur in Feb and May so we will bin based on Feb, May, and Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline - Binning Categorical Features:\n",
    "\n",
    "##Reduce categories for Operating Systems to the top 3 plus \"other\"\n",
    "### Operating Systems – is a categorical variable and most of the data (~95%) is in one of three operating systems (2, 1, 3). \n",
    "def binning_operating_systems(B):\n",
    "    if (B <= 3):\n",
    "        return(B)\n",
    "    else:\n",
    "        return(4) # creating a category of 4 for all Operating Systems > 3\n",
    "\n",
    "df['OperatingSystems_Bin']=df['OperatingSystems'].apply(binning_operating_systems)   # Creating a new column in the df\n",
    "\n",
    "\n",
    "      \n",
    "##Reduce categories for Browser to the top 3 plus \"other\"\n",
    "### Browser – is a categorical variable and most the data (~91%) come from three browsers (2, 1, 4).\n",
    "def binning_browser(B):\n",
    "    if (B == 3) or (B > 4): \n",
    "        return(3) \n",
    "    else:\n",
    "        return(B) \n",
    "\n",
    "df['Browser_Bin']=df['Browser'].apply(binning_browser)   # Creating a new column in the df\n",
    "      \n",
    "      \n",
    "##Reduce categories for Region to the top 4 plus \"other\"\n",
    "### Region – is a categorical variable for region from which the visitor came. The top four account for ~77% of the data (i.e., region 1, 3, 4, 2).\n",
    "def binning_region(B):\n",
    "    if (B <= 4):\n",
    "        return(B)\n",
    "    else:\n",
    "        return(5) # creating a category of 5 for all Regions > 4\n",
    "\n",
    "df['Region_Bin']=df['Region'].apply(binning_region)   # Creating a new column in the df\n",
    "\n",
    "      \n",
    "      \n",
    "##Reduce categories for TrafficType to the top 3 plus \"other\"\n",
    "### TrafficType – is a categorical variable to indicate how visitor arrived at website. The top three account for approximately 67% of the referrals (i.e., types 2, 1, 3).\n",
    "def binning_traffic_type(B):\n",
    "    if (B <= 3):\n",
    "        return(B)\n",
    "    else:\n",
    "        return(4) # creating a category of 4 for all Traffic Types > 3\n",
    "\n",
    "df['TrafficType_Bin']=df['TrafficType'].apply(binning_traffic_type)   # Creating a new column in the df\n",
    "\n",
    "\n",
    "##Create holiday/non-holiday bin for Feb/May = Holiday; others = Non-holiday\n",
    "### Months – is a boolean variable to the month of the internet visits. \n",
    "def holiday_bin_func(month) :\n",
    "    if month == 'May':\n",
    "        return int(1)\n",
    "    elif month == 'Feb':\n",
    "        return int(1)\n",
    "    else:\n",
    "        return int(0)\n",
    "    \n",
    "df['Holiday_Bin'] = df['Month'].apply(holiday_bin_func)\n",
    "\n",
    "##Reduce months to the top 4 in which there are transactions and \"other\"\n",
    "def month_bin_func(month) :\n",
    "    if month == 'May':\n",
    "        return int(5)\n",
    "    elif month == 'Nov':\n",
    "        return int(11)\n",
    "    elif month == 'Mar':\n",
    "        return int(3)\n",
    "    elif month == 'Dec':\n",
    "        return int(12)\n",
    "    else:\n",
    "        return int(0)\n",
    "    \n",
    "df['Month_Bin'] = df['Month'].apply(month_bin_func)\n",
    "\n",
    "\n",
    "##Encode month names to numerical representations\n",
    "\n",
    "def month_func(month) :\n",
    "    if month == 'Jan':\n",
    "        return int(1)\n",
    "    elif month == 'Feb':\n",
    "        return int(2)\n",
    "    elif month == 'Mar':\n",
    "        return int(3)\n",
    "    elif month == 'Apr':\n",
    "        return int(4)\n",
    "    elif month == 'May':\n",
    "        return int(5)\n",
    "    elif month == 'June':\n",
    "        return int(6)\n",
    "    elif month == 'Jul':\n",
    "        return int(7)\n",
    "    elif month == 'Aug':\n",
    "        return int(8)\n",
    "    elif month == 'Sep':\n",
    "        return int(9)\n",
    "    elif month == 'Oct':\n",
    "        return int(10)\n",
    "    elif month == 'Nov':\n",
    "        return int(11)\n",
    "    elif month == 'Dec':\n",
    "        return int(12)\n",
    "\n",
    "df['Month'] = df['Month'].apply(month_func)\n",
    "\n",
    "# validate that each bin function worked as intended\n",
    "display(df[['OperatingSystems', 'OperatingSystems_Bin', 'TrafficType', 'TrafficType_Bin', 'Browser', 'Browser_Bin', 'Region', 'Region_Bin', 'Month', 'Month_Bin','SpecialDay','Holiday_Bin']].sample(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chech the data types of the newly created features\n",
    "df[['OperatingSystems', 'OperatingSystems_Bin', 'TrafficType', 'TrafficType_Bin', 'Browser', 'Browser_Bin', 'Region', 'Region_Bin', 'Month', 'Month_Bin','SpecialDay','Holiday_Bin','Admin_per_Exit', 'Admin_per_Bounce',\\\n",
    "    'Bounce_Exit_Rate_Avg', 'Bounce_Exit_Rate_WeightedAvg', 'Bounce_per_Exit_Rate', 'Total_Duration','Total_Duration_Avg','Admin_Duration_percent_TotalDuration',\\\n",
    "           'Info_Duration_percent_TotalDuration','Product_Duration_percent_TotalDuration','TotalDuration_per_PageValues', 'Admin_per_PageValues', 'AdminDuration_per_PageValues', 'Informational_per_PageValues',\\\n",
    "           'Info_Duration_per_PageValues','ProductRelated_per_PageValues', 'Product_Duration_per_PageValues', 'Exit_per_PageValues', 'Bounce_per_PageValues']].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "    a. Outlier Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilize IQR method to address outliers\n",
    "\n",
    "def replace_columns_outliers_iqr(df, column_list): \n",
    "    for my_col in column_list:\n",
    "        Q1 = df[my_col].quantile(0.25)\n",
    "        Q3 = df[my_col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        u_bound_q3 = (Q3 + 1.5 * IQR)\n",
    "        l_bound_q1 = (Q1 - 1.5 * IQR)\n",
    "\n",
    "        df[my_col][df[my_col] > u_bound_q3] = u_bound_q3\n",
    "        df[my_col][df[my_col] < l_bound_q1] = l_bound_q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Address Outliers for all calculated variables using the IQR method\n",
    "\n",
    "calcualted_cols2 = calcualted_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del calcualted_cols2[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calcualted_cols2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_columns_outliers_iqr(df=df, column_list=calcualted_cols2) \n",
    "df[calcualted_cols2].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking to see if the created attributes have null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    b. Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting the data to be used for modeling\n",
    "display(df.dtypes)\n",
    "df_list =  df.columns\n",
    "df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = df_list.drop(['Revenue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Revenue before splitting the data to allow for modeling\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "enc = LabelEncoder()\n",
    "df['Revenue'] = enc.fit_transform(df['Revenue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data in to X and y\n",
    "X,y = df.loc[:,df_list],df.loc[:,'Revenue']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    c. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_col_list= X.columns.tolist()\n",
    "X_col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset X between categorical and continuous features\n",
    "\n",
    "X_continuous = ['Administrative','Administrative_Duration', 'Informational', 'Informational_Duration', 'ProductRelated', 'ProductRelated_Duration', 'BounceRates', 'ExitRates', \\\n",
    "                'PageValues', 'PageValues_Log', 'PageValues_Log10','Admin_per_Exit', 'Admin_per_Bounce', 'Bounce_Exit_Rate_Avg', 'Bounce_Exit_Rate_WeightedAvg', 'Bounce_per_Exit_Rate', 'Total_Duration', \\\n",
    "                'Total_Duration_Avg', 'Admin_Duration_percent_TotalDuration', 'Info_Duration_percent_TotalDuration', 'Product_Duration_percent_TotalDuration', 'TotalDuration_per_PageValues', \\\n",
    "                'Admin_per_PageValues', 'AdminDuration_per_PageValues', 'Informational_per_PageValues', 'Info_Duration_per_PageValues', 'ProductRelated_per_PageValues', 'Product_Duration_per_PageValues', \\\n",
    "                'Exit_per_PageValues', 'Bounce_per_PageValues', 'OperatingSystems_Bin', 'Browser_Bin', 'Region_Bin', 'TrafficType_Bin', 'Holiday_Bin', 'Month_Bin']\n",
    "\n",
    "X_categorical =['SpecialDay','Month','OperatingSystems','Browser','Region','TrafficType','VisitorType','Weekend', 'OperatingSystems_Bin', 'Browser_Bin','Region_Bin','TrafficType_Bin','Holiday_Bin','Month_Bin']\n",
    "\n",
    "X_continuous_df = X.loc[:,X_continuous]\n",
    "X_categorical_df = X.loc[:,X_categorical]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initially displaying the skewness of all attributes\n",
    "skew_df = pd.DataFrame(X_continuous_df.skew())\n",
    "\n",
    "#filter skew attributes by absolute values of 0.5\n",
    "skew_over = skew_df[(skew_df > 0.5).any(axis=1)]\n",
    "skew_under = skew_df[(skew_df < -0.5).any(axis=1)]\n",
    "display(skew_over.index)\n",
    "display(skew_under.index)\n",
    "total_skew_df = pd.concat([skew_over, skew_under])\n",
    "\n",
    "skew_cols = total_skew_df.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the list of cols to adjust for skewness\n",
    "\n",
    "for i in skew_cols:\n",
    "    X[i+'_skew'] = X[i]\n",
    "    \n",
    "    \n",
    "cols_to_skew = X.iloc[:,-27:].columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize using quantile_transform for columns that have skewness\n",
    "\n",
    "from sklearn.preprocessing import quantile_transform\n",
    "transformed_qt = quantile_transform(X[cols_to_skew],random_state=0,copy=True)\n",
    "transformed_qt_df = pd.DataFrame(transformed_qt,columns = cols_to_skew)\n",
    "X[cols_to_skew] = transformed_qt_df[cols_to_skew]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X[cols_to_skew].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Still_skew_df = pd.DataFrame(X[cols_to_skew].skew())\n",
    "\n",
    "#filter skew attributes by absolute values of 0.5\n",
    "still_skew_over = Still_skew_df[(Still_skew_df > 0.5).any(axis=1)]\n",
    "still_skew_under = Still_skew_df[(Still_skew_df < -0.5).any(axis=1)]\n",
    "display(still_skew_over.index)\n",
    "display(still_skew_under.index)\n",
    "\n",
    "col_still_skew_df = pd.concat([still_skew_over, still_skew_under])\n",
    "\n",
    "cols_to_skew_2 = col_still_skew_df.index.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_skew_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of columns that need to be transformed due to skewness\n",
    "\n",
    "# cols_to_skew_2 = ['Informational_skew','Informational_Duration_skew', 'PageValues_skew', 'PageValues_Log_skew', 'PageValues_Log10_skew', 'Holiday_Bin_skew']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize using PowerTransformer for remaining columns that continue to have skewness\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "transform = pt.fit(X[cols_to_skew_2])\n",
    "transformed = pt.transform((X[cols_to_skew_2]))\n",
    "transformed_df = pd.DataFrame(transformed,columns = cols_to_skew_2)\n",
    "X[cols_to_skew_2] = transformed_df[cols_to_skew_2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for remaining skewness\n",
    "display(X[cols_to_skew_2].skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - After two methods for correcting for skewness we are left with 8 attributes whose distributions remain skewed.\n",
    "    - We have decided to proceed without further adjustment to these attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visually chack for the columns that need to be rescaled based on a max higher than 1\n",
    "display(X.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review data types to ensure all data is processed for modeling\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the categorical variables that remain bool or object for modeling\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "enc = LabelEncoder()\n",
    "X['VisitorType'] = enc.fit_transform(X['VisitorType'])\n",
    "X['Weekend'] = enc.fit_transform(X['Weekend'])\n",
    "#X['Revenue'] = enc.fit_transform(X['Revenue'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[['VisitorType','Weekend']].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    d. Rescaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the attributes that have a range outside of zero to one (0 - 1)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scale = scaler.fit(X)\n",
    "scaled = scaler.transform(X)\n",
    "scaled_df = pd.DataFrame(scaled,columns = col_list)\n",
    "X_scaled = scaled_df\n",
    "# X[scale_cols] = scaled_df[scale_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate that the scaler worked as intended\n",
    "X_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    f. Naive Model/Baseline Model\n",
    ">   <b> Assumption: </b> All features are usefull for Y1 & Y2!\n",
    "<br><b> Calculate: </b> How many times are you right?\n",
    "<br><b> Reason: </b> This can be set as the baseline for our accuracy of Y1 and Y2 - the computer model should at least beat this in order for it to be better than guessing.\n",
    "<br><b> Answer: </b> \n",
    "* <b> Y: </b> 71% accuracy - Original Base at time of Data Audit Report\n",
    "* <b> Y: </b> 87% accuracy with an AUC of 89 at time of Inital Data Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resplit based on additional data prep completed post initial split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y,test_size=0.3,random_state=500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Gaussian Classifier\n",
    "gnb1 = GaussianNB()\n",
    "\n",
    "#Train the model using the training sets\n",
    "gnb1.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = gnb1.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "display(cnf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUC for y Base model\n",
    "y_pred_proba = gnb1.predict_proba(X_test)[::,1]\n",
    "fpr_base, tpr_base, _ = metrics.roc_curve(y_test, y_pred_proba)\n",
    "auc_base = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr_base,tpr_base,label=\"auc=\"+str(auc_base))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "print(\"\")\n",
    "\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix Y1', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    g. Reviewing Correlations to Identify Highly Related/Correlated Features to Avoid in Analysis:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define new dataframe, df_Prepped, which contains all the features, as adjusted per the Data Preparation above, along with the two target variables from the initial dataframe\n",
    "\n",
    "df_Prepped = pd.concat([X,y],axis=1)\n",
    "\n",
    "#check new dataframe, df_Prepped                \n",
    "df_Prepped.head()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Heatmap for the dataframe\n",
    "spearman =df_Prepped.corr(method ='spearman')\n",
    "plt.figure(figsize=(50,25))\n",
    "sns.heatmap(spearman, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = pd.DataFrame(X.corrwith(df_Prepped['Revenue']))\n",
    "\n",
    "#filter skew attributes by absolute values of 0.5\n",
    "corr_over = corr_df[(corr_df > 0.09).any(axis=1)]\n",
    "corr_under = corr_df[(corr_df < -0.09).any(axis=1)]\n",
    "display(corr_over.index)\n",
    "display(corr_under.index)\n",
    "\n",
    "corr_with_df = pd.concat([corr_over, corr_under])\n",
    "\n",
    "model_cols = corr_with_df.index.tolist()\n",
    "model_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_Features_df = X[model_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determined possible features list by removing corresponding, duplicative features that were not adjusted for skewness\n",
    " \n",
    "possible_Features_list =['Month',\n",
    " 'Administrative_skew',\n",
    " 'Administrative_Duration_skew',\n",
    " 'Informational_skew',\n",
    " 'Informational_Duration_skew',\n",
    " 'ProductRelated_skew',\n",
    " 'ProductRelated_Duration_skew',\n",
    " 'PageValues_skew',\n",
    " 'PageValues_Log_skew',\n",
    " 'PageValues_Log10_skew',\n",
    " 'Admin_per_Exit_skew',\n",
    " 'Admin_per_Bounce_skew',\n",
    " 'Total_Duration_skew',\n",
    " 'Total_Duration_Avg_skew',\n",
    " 'Bounce_per_Exit_Rate_skew',\n",
    " 'VisitorType',\n",
    " 'Exit_per_PageValues',\n",
    " 'Bounce_per_PageValues',\n",
    " 'BounceRates_skew',\n",
    " 'ExitRates_skew',\n",
    " 'Bounce_Exit_Rate_Avg_skew',\n",
    " 'Bounce_Exit_Rate_WeightedAvg_skew',\n",
    " 'Info_Duration_percent_TotalDuration_skew',\n",
    " 'TotalDuration_per_PageValues_skew',\n",
    " 'Admin_per_PageValues_skew',\n",
    " 'AdminDuration_per_PageValues_skew',\n",
    " 'ProductRelated_per_PageValues_skew',\n",
    " 'Product_Duration_per_PageValues_skew','Revenue']    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Heatmap for the dataframe\n",
    "possible_Corr_df = df_Prepped.loc[:,possible_Features_list]\n",
    "possible_Corr_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman_possible =possible_Corr_df.corr(method ='spearman')\n",
    "plt.figure(figsize=(30,15))\n",
    "sns.heatmap(spearman_possible, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the above:\n",
    "# - Need to choose only one of the variables for which there is a corresponding duration variable - namely:\n",
    "#    ~Administrative_skew or Administrative_Duration_skew (correlation 0.94)\n",
    "#    ~Informational_skew or Informational_Duration_skew (correlation 0.95)\n",
    "#    ~ProductRelated_skew or Product_RelatedDuration_skew (correlation 0.88)\n",
    "#    **Initial decision: utilize Administrative_skew and ProductRelated_Duration_skew as they are more highly correlated with the target variable;\n",
    "#      utilize Informational_skew because Informational_Duration_skew is incorporated into the Total_Duration variables (discussed below)\n",
    "\n",
    "# - Need to choose only one of the variables from pairs that capture related information - namely:\n",
    "#    ~BounceRates_skew or ExitRates_skew (correlation 0.60)\n",
    "#    ~Bounce_Exit_Rate_Avg_skew or Bounce_per_Exit_Rate_skew (correlation 0.62)\n",
    "#    ~Total_Duration_skew or Total_Duration_Avg_Skew (correlation 1.00)\n",
    "#    **Initial decision: For the first two, utilize the ones more strongly correlated with the corresponding Y values - namely: ExitRates_skew & Bounce_Exit_Rate_Avg_skew;\n",
    "#      In the case of the third variable pair, we opt to use the value for Total_Duration_skew (rather than the averaged value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered out intercorrelated featues for feature importance\n",
    "feature_importance_list = ['Month', 'Administrative_skew','ProductRelated_Duration_skew', 'Informational_skew','PageValues_skew','Admin_per_Exit_skew','Admin_per_Bounce_skew',\\\n",
    "                           'Bounce_Exit_Rate_WeightedAvg_skew', 'VisitorType','Exit_per_PageValues','Bounce_per_PageValues','Admin_per_PageValues_skew','TotalDuration_per_PageValues_skew',\\\n",
    "                           'ProductRelated_per_PageValues_skew']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    h. Split the data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the df_Models dataset\n",
    "X_Models,y_Models = possible_Corr_df.loc[:,feature_importance_list],possible_Corr_df.iloc[:,-1]\n",
    "\n",
    "# Set the training at 30% (as above for baseline) given the modest size of the dataset (~12,000 observations)\n",
    "X_Models_train, X_Models_test, y_Models_train, y_Models_test = train_test_split(X_Models, y_Models, test_size=0.3,random_state=500) \n",
    "\n",
    "#print out the first five rows of the training data\n",
    "display(X_Models_train.head(),y_Models_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the GN model without feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the metrics class\n",
    "from sklearn import metrics\n",
    "\n",
    "# import other required modules for confusion matrices\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###1. Step 1: Specify the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2. Steps 2-4: Generate Test Data, Build the Models & Assess the Models for y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model using the training sets - for y2 (Sale)\n",
    "gnb.fit(X_Models_train, y_Models_train)\n",
    "\n",
    "#Predict the response for test dataset for y2\n",
    "y_NB_pred = gnb.predict(X_Models_test)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "# Accuracy for y2\n",
    "print(\"y Accuracy:\",metrics.accuracy_score(y_Models_test, y_NB_pred))\n",
    "print(\"\")\n",
    "\n",
    "#Can use classification report to assess model adequacy, too\n",
    "print(metrics.classification_report(y_Models_test, y_NB_pred, labels=class_names))\n",
    "\n",
    "#AUC for y\n",
    "y_NB_pred_proba = gnb.predict_proba(X_Models_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_Models_test,  y_NB_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_Models_test, y_NB_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "print(\"\")\n",
    "\n",
    "#Print Confusion Matrix\n",
    "cnf_matrix = metrics.confusion_matrix(y_Models_test, y_NB_pred)\n",
    "cnf_matrix\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conclusion - With accuracy of 81% for each y1 and y2, the Naive Bayes model is superior to the base model which had an accuracy of 71% for each target.\n",
    "#However, there is still room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV: Data Analysis/Modeling <a name=\"part4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### add resampled data into models **\n",
    "##### can also delete all of the y2 models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Naive Bayes Model (NB)\n",
    ">   <b> Assumption: </b> Model features all are independent; we have included all features we believe independent per the immediately preceding correlation analysis\n",
    "<br><b> Calculate Accuracy: </b> How many times are you right?\n",
    "<br><b> Answer: </b> \n",
    "* <b> Y1 </b>: 81.21% accuracy (AUC = 0.8741)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###0. Step 0: Import Needed Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the metrics class\n",
    "from sklearn import metrics\n",
    "\n",
    "# import other required modules for confusion matrices\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###1. Step 1: Specify the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2. Steps 2-4: Generate Test Data, Build the Models & Assess the Models for y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model using the training sets - for y2 (Sale)\n",
    "gnb.fit(X_Models_train, y_Models_train)\n",
    "\n",
    "#Predict the response for test dataset for y2\n",
    "y_NB_pred = gnb.predict(X_Models_test)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "# Accuracy for y2\n",
    "print(\"y Accuracy:\",metrics.accuracy_score(y_Models_test, y_NB_pred))\n",
    "print(\"\")\n",
    "\n",
    "#Can use classification report to assess model adequacy, too\n",
    "print(metrics.classification_report(y_Models_test, y_NB_pred, labels=class_names))\n",
    "\n",
    "#AUC for y\n",
    "y_NB_pred_proba = gnb.predict_proba(X_Models_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_Models_test,  y_NB_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_Models_test, y_NB_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "print(\"\")\n",
    "\n",
    "#Print Confusion Matrix\n",
    "cnf_matrix = metrics.confusion_matrix(y_Models_test, y_NB_pred)\n",
    "cnf_matrix\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conclusion - With accuracy of 81% for each y1 and y2, the Naive Bayes model is superior to the base model which had an accuracy of 71% for each target.\n",
    "#However, there is still room for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MD's Modeling Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Decision Tree Model (DT)\n",
    ">   <b> Assumption: </b> All features are usefull for Y1 & Y2!\n",
    "<br><b> Calculate: </b> How many times are you right?\n",
    "<br><b> Reason: </b> This can be set as the baseline for our accuracy of Y1 and Y2 - the computer model should at least beat this in order for it to be better than guessing.\n",
    "<br><b> Answer: </b> \n",
    "* <b> Y1 </b>: 85.56% accuracy\n",
    "* <b> Y2: </b> 85.45% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to take look at the models df to see if everything is correct\n",
    "#df_Models.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_Models_train,y_Models_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_DT_pred = clf.predict(X_Models_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_Models_test, y_DT_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assess the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the Classification Report\n",
    "print(metrics.classification_report(y_Models_test, y_DT_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the Confusion Matrix\n",
    "print(metrics.confusion_matrix(y_Models_test, y_DT_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUC for y\n",
    "y_DT_pred_proba = clf.predict_proba(X_Models_test)[::,1]\n",
    "fpr_DT, tpr_DT, _ = metrics.roc_curve(y_Models_test,  y_DT_pred_proba)\n",
    "auc_DT = metrics.roc_auc_score(y_Models_test, y_DT_pred_proba)\n",
    "plt.plot(fpr_DT,tpr_DT,label=\"auc=\"+str(auc_DT))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "print(\"\")\n",
    "\n",
    "#Print Confusion Matrix\n",
    "cnf_matrix_DT = metrics.confusion_matrix(y_Models_test, y_DT_pred)\n",
    "cnf_matrix_DT\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_DT), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix DT', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "85.45% is much better than the baseline model of 70%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *** Attempted Model Optimization - Not 100% if I optimized these correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### y2 (Sale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to optimize Decision Tree Model by adding in the parameters \"entropy\" (information gain selection measure) and \"max depth=3\"\n",
    "# # Did this in order to reduce to complexity of the Decision Tree, in hopes that it will yield better results\n",
    "# Create Decision Tree classifer object\n",
    "clf2 = DecisionTreeClassifier(criterion = \"entropy\", max_depth=3)\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf2 = clf2.fit(X_Models_train,y_Models_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_DT_pred2 = clf2.predict(X_Models_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_Models_test, y_DT_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the Classification Report\n",
    "print(metrics.classification_report(y_Models_test, y_DT_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the Confusion Matrix\n",
    "print(metrics.confusion_matrix(y_Models_test, y_DT_pred2))\n",
    "\n",
    "#AUC for y\n",
    "y_DT_pred_proba2 = clf2.predict_proba(X_Models_test)[::,1]\n",
    "fpr_DT2, tpr_DT2, _ = metrics.roc_curve(y_Models_test,  y_DT_pred_proba2)\n",
    "auc_DT2 = metrics.roc_auc_score(y_Models_test, y_DT_pred_proba2)\n",
    "plt.plot(fpr_DT2,tpr_DT2,label=\"auc=\"+str(auc_DT2))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "print(\"\")\n",
    "\n",
    "#Print Confusion Matrix\n",
    "cnf_matrix_DT2 = metrics.confusion_matrix(y_Models_test, y_DT_pred2)\n",
    "cnf_matrix_DT2\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_DT2), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix DT 2', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Not sure if the optimized model is overfitting, but 89.53% accuracy is better than the base Decision Tree Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Random Forest Model (RF)\n",
    ">   <b> Assumption: </b> All features are usefull for Y1 & Y2!\n",
    "<br><b> Calculate: </b> How many times are you right?\n",
    "<br><b> Reason: </b> This can be set as the baseline for our accuracy of Y1 and Y2 - the computer model should at least beat this in order for it to be better than guessing.\n",
    "<br><b> Answer: </b> \n",
    "* <b> Y1 </b>: 90% accuracy\n",
    "* <b> Y2: </b> 90% accuracy (not sure if Y2 result should be the same as Y1 but could make sense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Classifier\n",
    "#Import scikit-learn dataset library\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Model for y2 (Sale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Random Forest Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_Models_test)\n",
    "rfc.fit(X_Models_train,y_Models_train)\n",
    "\n",
    "y_RF_pred = rfc.predict(X_Models_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of the Classification Report\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_Models_test, y_RF_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of the Confusion Matrix\n",
    "print(confusion_matrix(y_Models_test, y_RF_pred))\n",
    "\n",
    "#AUC for y\n",
    "y_RF_pred_proba = rfc.predict_proba(X_Models_test)[::,1]\n",
    "fpr_RF, tpr_RF, _ = metrics.roc_curve(y_Models_test,  y_RF_pred_proba)\n",
    "auc_RF = metrics.roc_auc_score(y_Models_test, y_RF_pred_proba)\n",
    "plt.plot(fpr_RF,tpr_RF,label=\"auc=\"+str(auc_RF))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "print(\"\")\n",
    "\n",
    "#Print Confusion Matrix\n",
    "cnf_matrix_RF = metrics.confusion_matrix(y_Models_test, y_RF_pred)\n",
    "cnf_matrix_RF\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_RF), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix RF', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 90% accuracy for both Random Forest Models, so far better than the Decision Tree Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More to do... Can do Feature Importance using scikit_learn - have the code for it but couldn't get it to run ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Attempt on the feature selection using scikit-learn\n",
    "# # import the package\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# #Create a Gaussian Classifier\n",
    "# clf=RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# #Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "# clf.fit(X_Models_train,y_Models_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# feature_imp = pd.Series(clf.feature_importances_,index=y_Models_test).sort_values(ascending=False)\n",
    "# feature_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Support Vector Machines (SVM) Classification Model (SVC)\n",
    ">   <b> Assumption: </b> All features are usefull for Y1 & Y2!\n",
    "<br><b> Calculate: </b> How many times are you right?\n",
    "<br><b> Reason: </b> This can be set as the baseline for our accuracy of Y1 and Y2 - the computer model should at least beat this in order for it to be better than guessing.\n",
    "<br><b> Answer: </b> \n",
    "* <b> Y1 </b>: 70.78% accuracy\n",
    "* <b> Y2: </b> 70.78% accuracy (not sure if Y2 result should be the same as Y1 but could make sense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###1. Step 1: Specify the Model\n",
    "from sklearn import svm\n",
    "svc = svm.SVC(kernel='rbf',cache_size=7000,gamma= 'auto', C=5, probability =True, degree = 5) # gamma= 0.001 , kernel='poly', 'rbf',‘linear’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###2. Step 2: Generate Test Data\n",
    "# X_train_svc, X_test_svc, y1_train_svc, y1_test_svc, y2_train_svc, y2_test_svc = train_test_split(X, y1,y2, test_size=0.3,random_state=1000) \n",
    "# X_Models_train, X_Models_test, y1_Models_train, y1_Models_test, y2_Models_train, y2_Models_test\n",
    "#Standard Scale the data to allow for better SVC model performance\n",
    "# scaler = StandardScaler().fit(X_Models_train) \n",
    "# standardized_X_svc = scaler.transform(X_Models_train) \n",
    "# standardized_X_test_svc = scaler.transform(X_Models_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###3. Step 3: Build the Model\n",
    "#Train the model using the training sets\n",
    "svc.fit(X_Models_train, y_Models_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred_svc = svc.predict(X_Models_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###4. Step 4: Assess the Model\n",
    "print(\"Accuracy_svc:\",metrics.accuracy_score(y_Models_test, y_pred_svc))\n",
    "cnf_matrix_svc = metrics.confusion_matrix(y_Models_test, y_pred_svc)\n",
    "\n",
    "print(cnf_matrix_svc)\n",
    "\n",
    "\n",
    "#AUC for y1\n",
    "y_SVM_pred_proba = svc.predict_proba(X_Models_test)[::,1]\n",
    "fpr_SVM, tpr_SVM, _ = metrics.roc_curve(y_Models_test,  y_SVM_pred_proba)\n",
    "auc_SVM = metrics.roc_auc_score(y_Models_test, y_SVM_pred_proba)\n",
    "plt.plot(fpr_SVM,tpr_SVM,label=\"auc=\"+str(auc_SVM))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "print(\"\")\n",
    "\n",
    "class_names_svc=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks_svc = np.arange(len(class_names_svc))\n",
    "plt.xticks(tick_marks_svc, class_names_svc)\n",
    "plt.yticks(tick_marks_svc, class_names_svc)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_svc), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix svc', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "print(classification_report(y_Models_test, y_pred_svc))\n",
    "print(\"Accuracy_svc:\",metrics.accuracy_score(y_Models_test, y_pred_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. XGBoost Model (XGB)\n",
    ">   <b> Assumption: </b> XXXAll features are usefull for Y1 & Y2!XXX\n",
    "<br><b> Calculate: </b> How many times are you right?\n",
    "<br><b> Reason: </b> XXXThis can be set as the baseline for our accuracy of Y1 and Y2 - the computer model should at least beat this in order for it to be better than guessing.XXX\n",
    "<br><b> Answer: </b> \n",
    "* <b> Y2 </b>: 90.19% accuracy\n",
    "* <b> Y2: </b> XXX94.97 RMSEXXX - not sure this is right!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###0. Step 0: Import Needed Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Needed Packages\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Do preliminary work\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###1. Step 1: Specify the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate an XGBoost Classifer Model - for y1 (No_Sale)\n",
    "XGB_class = xgb.XGBClassifier(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 5, alpha = 10, n_estimators = 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2. Steps 2-4: Generate Test Data, Build the Models & Assess the Models for y2 (Sale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put Data into structure for XGBoost- for y2 (Sale) \n",
    "data_dmatrix = xgb.DMatrix(data=X_Models,label=y_Models)\n",
    "\n",
    "#Train the model using the training sets for y1\n",
    "XGB_class.fit(X_Models_train, y_Models_train)\n",
    "\n",
    "#Predict the response for test dataset for y1\n",
    "y_XGB_pred = XGB_class.predict(X_Models_test)\n",
    "\n",
    "#Calculate RMSE for y2\n",
    "rmse_XGB = np.sqrt(mean_squared_error(y_Models_test, y_XGB_pred))\n",
    "print(\"XGBoost's RMSE for y2 is: %f\" % (rmse_XGB))\n",
    "\n",
    "#Create error ratio to evaluate results for y2\n",
    "target_range_XGB = y_Models.max() - y_Models.min()\n",
    "print(\"XGB target range is: %f\" % (target_range_XGB))\n",
    "error_ratio_XGB = rmse_XGB/target_range_XGB\n",
    "print(\"XGBoost's Error Ratio for y2 is: %f\" % (error_ratio_XGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ISSUE WITH THE ABOVE - HOW COME THE TARGET RANGE IS 1? DOES IT HAVE TO DO WITH 0/1 STATUS OF TARGET VARIABLE???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Accuracy, how often is the classifier correct?\n",
    "# Accuracy for y2\n",
    "print(\"y Accuracy:\",metrics.accuracy_score(y_Models_test, y_XGB_pred))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "#Can use classification report to assess model adequacy, too\n",
    "print(metrics.classification_report(y_Models_test, y_XGB_pred, labels=class_names))\n",
    "\n",
    "#AUC for y1\n",
    "y_XGB_pred_proba = XGB_class.predict_proba(X_Models_test)[::,1]\n",
    "fpr_XGB, tpr_XGB, _ = metrics.roc_curve(y_Models_test,  y_XGB_pred_proba)\n",
    "auc_XGB = metrics.roc_auc_score(y_Models_test, y_XGB_pred_proba)\n",
    "plt.plot(fpr_XGB,tpr_XGB,label=\"auc=\"+str(auc_XGB))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "print(\"\")\n",
    "\n",
    "#Print Confusion Matrix\n",
    "cnf_matrix_XGB = metrics.confusion_matrix(y_Models_test, y_XGB_pred)\n",
    "cnf_matrix_XGB\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_XGB), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix XGB', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More to go...including: k-fold cross-validation, visualization for feature importance & hyper-parameter tuning to improve model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F. Neural Network Model (NN)\n",
    ">   <b> Assumption: </b> All features are usefull for Y1 & Y2!\n",
    "<br><b> Calculate: </b> How many times are you right?\n",
    "<br><b> Reason: </b> This can be set as the baseline for our accuracy of Y1 and Y2 - the computer model should at least beat this in order for it to be better than guessing.\n",
    "<br><b> Answer: </b> \n",
    "* <b> Y1 </b>: 90.26% accuracy\n",
    "* <b> Y2: </b> -% accuracy (not sure if Y2 result should be the same as Y1 but could make sense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_Models_train, X_Models_test, y1_Models_train, y1_Models_test, y2_Models_train, y2_Models_test\n",
    "\n",
    "\n",
    "###1. Step 1: Specify the Model\n",
    "# Import the model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Initializing the multilayer perceptron\n",
    "# mlp = MLPClassifier(hidden_layer_sizes = (3,1),solver='sgd',learning_rate_init= 0.01, max_iter=50)\n",
    "mlp= MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto', beta_1=0.9, \n",
    "beta_2=0.999, early_stopping=False, epsilon=1e-08,       \n",
    "hidden_layer_sizes=(7,3), learning_rate='adaptive',      \n",
    "learning_rate_init=0.01, max_iter=10000, momentum=0.9,       \n",
    "nesterovs_momentum=True, power_t=0.5, random_state=1000,       \n",
    "shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,       \n",
    "verbose=False, warm_start=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###2. Step 2: Generate Test Data\n",
    "# Train the model\n",
    "mlp.fit(X_Models_train, y_Models_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###3. Step 3: Build the Model\n",
    "y_pred_nn = mlp.predict(X_Models_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###4. Step 4: Assess the Model\n",
    "# Score takes a feature matrix X_test and the expected target values y_test. \n",
    "# Predictions for X_test are compared with y_test\n",
    "\n",
    "print(\"MLP score is\",mlp.score(X_Models_test,y_Models_test))\n",
    "\n",
    "# Accuracy for y NN\n",
    "#print(\"y Accuracy NN:\",metrics.accuracy_score(y_Models_test, y_XGB_pred))\n",
    "#print(\"\")\n",
    "\n",
    "###4. Step 4: Assess the Model\n",
    "print(\"Accuracy_nn:\",metrics.accuracy_score(y_Models_test, y_pred_nn))\n",
    "cnf_matrix_nn = metrics.confusion_matrix(y_Models_test, y_pred_nn)\n",
    "\n",
    "print(cnf_matrix_nn)\n",
    "\n",
    "# Plot AOC\n",
    "y_pred_proba_nn = mlp.predict_proba(X_Models_test)[::,1]\n",
    "fpr_nn, tpr_nn, _ = metrics.roc_curve(y_Models_test,  y_pred_proba_nn)\n",
    "auc_nn = metrics.roc_auc_score(y_Models_test, y_pred_proba_nn)\n",
    "plt.plot(fpr_nn,tpr_nn,label=\"data 1, auc=\"+str(auc_nn))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "class_names_nn=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks_nn = np.arange(len(class_names_nn))\n",
    "plt.xticks(tick_marks_nn, class_names_nn)\n",
    "plt.yticks(tick_marks_nn, class_names_nn)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_nn), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix NN', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "print(classification_report(y_Models_test, y_pred_nn))\n",
    "print(\"Accuracy_svc:\",metrics.accuracy_score(y_Models_test, y_pred_nn))\n",
    "\n",
    "#Plot AOC\n",
    "# y_pred_proba_nn = mlp.predict_proba(X_Models_test)[::,1]\n",
    "# fpr_nn, tpr_nn, _ = metrics.roc_curve(y_Models_test,  y_pred_proba_nn)\n",
    "# auc_nn = metrics.roc_auc_score(y_Models_test, y_pred_proba_nn)\n",
    "# plt.plot(fpr_nn,tpr_nn,label=\"data 1, auc=\"+str(auc_nn))\n",
    "# plt.legend(loc=4)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">   <b> Assumption: </b> All features are usefull for Y\n",
    "<br><b> Calculate: </b> How many times are you right?\n",
    "<br><b> Reason: </b> This can be set as the baseline for our accuracy of Y - the computer model should at least beat this in order for it to be better than guessing.\n",
    "<br><b> Answer: </b> \n",
    "* <b> Y </b>: 90.3% accuracy - about the same as our other best scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the model\n",
    "# import the class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate the model (using the default parameters)\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# fit the model with data\n",
    "logreg.fit(X_Models_train,y_Models_train)\n",
    "\n",
    "#\n",
    "y_LR_pred=logreg.predict(X_Models_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the metrics class for the Confusion Matrix\n",
    "from sklearn import metrics\n",
    "cnf_matrix_LogR = metrics.confusion_matrix(y_Models_test, y_LR_pred)\n",
    "cnf_matrix_LogR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Confusion Matrix\n",
    "\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_LogR), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix Log Reg', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "y_LR_pred_proba = logreg.predict_proba(X_Models_test)[::,1]\n",
    "fpr_LR, tpr_LR, _ = metrics.roc_curve(y_Models_test,  y_LR_pred_proba)\n",
    "auc_LR = metrics.roc_auc_score(y_Models_test, y_LR_pred_proba)\n",
    "plt.plot(fpr_LR,tpr_LR,label=\"data 1, auc=\"+str(auc_LR))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_Models_test, y_LR_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_Models_test, y_LR_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_Models_test, y_LR_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#90.3% accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_kmeans = np.array(X_Models_train)\n",
    "y_kmeans = np.array(y_Models_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "\n",
    "# load the model\n",
    "kmeans = KMeans(n_clusters=2, max_iter=600, algorithm = 'auto') # 2 clusters, sale or no sale\n",
    "kmeans.fit(X_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "correct = 0\n",
    "for i in range(len(X_kmeans)):\n",
    "    predict_me = np.array(X_kmeans[i].astype(float))\n",
    "    predict_me = predict_me.reshape(-1, len(predict_me))\n",
    "    prediction = kmeans.predict(predict_me)\n",
    "    if prediction[0] == y_kmeans[i]:\n",
    "        correct += 1\n",
    "\n",
    "print(correct/len(X_kmeans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling & Feature Importance: Next Steps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the models with reshaped the data due to inbalance in 'Y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over sample the data using SMOTE\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE # doctest: +NORMALIZE_WHITESPACE\n",
    "# X, y = make_classification(n_classes=2, class_sep=2,\n",
    "#     weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,\n",
    "#     n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=2019)\n",
    "print('Original dataset shape %s' % Counter(y_Models_train))\n",
    "#Original dataset shape Counter({1: 900, 0: 100})\n",
    "sm = SMOTE(random_state=42)\n",
    "over_sampl_X_Models_train, over_sampled_y_Models_train = sm.fit_resample(X_Models_train, y_Models_train) #DWM Note: Are we SURE on \"Models\" in last X_train, y2_train - per SMOTE doc from Tao\n",
    "print('Resampled dataset shape %s' % Counter(over_sampled_y_Models_train))\n",
    "#Resampled dataset shape Counter({0: 900, 1: 900})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Print first 5 rows\n",
    "display(over_sampled_X_Models_train.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under sample the data using Near Miss\n",
    "# https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.under_sampling.NearMiss.html\n",
    "\n",
    "# X, y = make_classification(n_classes=2, class_sep=2,\n",
    "#     weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,\n",
    "#     n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=2019)\n",
    "print('Original dataset shape %s' % Counter(y_Models_train))\n",
    "#Original dataset shape Counter({1: 900, 0: 100})\n",
    "nm = NearMiss(sampling_strategy='all')\n",
    "under_sampled_X_Models_train, under_sampled_y_Models_train = nm.fit_resample(X_Models_train, y_Models_train) #DWM Note: Are we SURE on \"Models\" in last X_train, y2_train - per SMOTE doc from Tao\n",
    "print('Under sampled dataset shape %s' % Counter(under_sampled_y_Models_train))\n",
    "#Resampled dataset shape Counter({0: 900, 1: 900})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print first 5 rows\n",
    "display(under_sampled_X_Models_train.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Attempt on the feature selection using scikit-learn\n",
    "# # import the package\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# #Create a Gaussian Classifier\n",
    "# clf=RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# #Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "# clf.fit(X_Models_train,y_Models_train)\n",
    "\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# feature_imp = pd.Series(clf.feature_importances_,index=y_Models_test).sort_values(ascending=False)\n",
    "# feature_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    d. One Hot Encoding & Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pipeline - Encoding:\n",
    "# oh_enc = OneHotEncoder(sparse=False) # initializing One-Hot Encoder Function\n",
    "\n",
    "# ## One-Hot for VisitorType to create new columns for 'Returning_Visitor' and 'New_Visitor'\n",
    "# encoder_visitortype = X[['VisitorType']].values\n",
    "# visitortype_encoded = encoder_visitortype.reshape(len(encoder_visitortype), 1)\n",
    "# visitortype_onehot_encoded = oh_enc.fit_transform(visitortype_encoded)\n",
    "# visitortype_onehot_df = pd.DataFrame(visitortype_onehot_encoded, columns = [\"Returning_Visitor\", \"New_Visitor\"])\n",
    "# visitortype_onehot_df.head()\n",
    "\n",
    "# ## Creating list for newly-created columns for VisitorType\n",
    "# visitor_list = visitortype_onehot_df.columns\n",
    "\n",
    "\n",
    "# ## One-Hot for Weekend to create new columns for 'Is_Weekend' and 'Not_Weekend'\n",
    "# encoder_weekend = df[['Weekend']].values\n",
    "# weekend_encoded = encoder_weekend.reshape(len(encoder_weekend), 1)\n",
    "# weekend_onehot_encoded = oh_enc.fit_transform(weekend_encoded)\n",
    "# weekend_onehot_df = pd.DataFrame(weekend_onehot_encoded, columns = [\"Not_Weekend\", \"Is_Weekend\"])\n",
    "# weekend_onehot_df.head()\n",
    "\n",
    "# ## Creating list for newly-created columns for Weekend\n",
    "# weekend_list = weekend_onehot_df.columns\n",
    "\n",
    "\n",
    "# ## Combine Holiday Seasons Months\n",
    "# ## One-Hot for Month to create new columns for \"Month_May\", \"Month_Nov\",\"Month_Mar\", \"Month_Dec\", \"Month_Other\"\n",
    "# encoder_month = df[['Month_Bin']].values\n",
    "# month_onehot_encoded = oh_enc.fit_transform(encoder_month)\n",
    "# month_onehot_df = pd.DataFrame(month_onehot_encoded,  columns = [\"Month_May\", \"Month_Nov\",\"Month_Mar\", \"Month_Dec\", \"Month_Other\"])\n",
    "# month_onehot_df.head()\n",
    "\n",
    "# ## Creating list for newly-created columns for Month\n",
    "# month_list = month_onehot_df.columns\n",
    "# df [month_list] = month_onehot_df.loc[:,month_list]\n",
    "# df['VisitorType'] = df['VisitorType'].replace('Other','Returning_Visitor')\n",
    "\n",
    "# # ## We can delete the revenue encoding:\n",
    "# # ## One-Hot Encoding for Y to separate into two new columns for 'Sale' and 'No_Sale'\n",
    "# # encoder_revenue = df[['Revenue']].values\n",
    "# # revenue_encoded = encoder_revenue.reshape(len(encoder_revenue), 1)\n",
    "# # revenue_onehot_encoded = oh_enc.fit_transform(revenue_encoded)\n",
    "# # revenue_onehot_df = pd.DataFrame(revenue_onehot_encoded, columns = [\"No_Sale\", \"Sale\"])\n",
    "# # revenue_onehot_df.head(30)\n",
    "\n",
    "# # ## Creating list for newly-created columns for Y\n",
    "# # rev_list = revenue_onehot_df.columns\n",
    "\n",
    "# ## Code to add newly created columns to the df\n",
    "# X[month_list] = month_onehot_df.loc[:,month_list]\n",
    "# X[visitor_list] = visitortype_onehot_df.loc[:,visitor_list]\n",
    "# X[weekend_list] = weekend_onehot_df.loc[:,weekend_list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Naive Bayes Model (NB) Over Under Sampled\n",
    ">   <b> Assumption: </b> Model features all are independent; we have included all features we believe independent per the immediately preceding correlation analysis\n",
    "<br><b> Calculate Accuracy: </b> How many times are you right?\n",
    "<br><b> Answer: </b> \n",
    "* <b> Y1 </b>: 81.21% accuracy (AUC = 0.8741)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###0. Step 0: Import Needed Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the metrics class\n",
    "from sklearn import metrics\n",
    "\n",
    "# import other required modules for confusion matrices\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###1. Step 1: Specify the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "gnbo = GaussianNB()\n",
    "gnbu = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2. Steps 2-4: Generate Test Data, Build the Models & Assess the Models for y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model using the training sets - for y2 (Sale)\n",
    "gnbo.fit(over_sampl_X_Models_train, over_sampled_y_Models_train)\n",
    "gnbu.fit(under_sampled_X_Models_train, under_sampled_y_Models_train)\n",
    "\n",
    "\n",
    "#Predict the response for test dataset for y2\n",
    "y_NB_pred_over_s = gnbo.predict(X_Models_test)\n",
    "y_NB_pred_under_s = gnbu.predict(X_Models_test)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "# Accuracy for y2\n",
    "print(\"y Accuracy:\",metrics.accuracy_score(y_Models_test, y_NB_pred_over_s))\n",
    "print(\"\")\n",
    "print(\"y Accuracy:\",metrics.accuracy_score(y_Models_test, y_NB_pred_under_s))\n",
    "print(\"\")\n",
    "\n",
    "#Can use classification report to assess model adequacy, too\n",
    "print(metrics.classification_report(y_Models_test, y_NB_pred_over_s, labels=class_names))\n",
    "print(metrics.classification_report(y_Models_test, y_NB_pred_under_s, labels=class_names))\n",
    "\n",
    "#AUC for y over sampled\n",
    "y_NB_pred_proba = gnbo.predict_proba(X_Models_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_Models_test,  y_NB_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_Models_test, y_NB_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "print(\"\")\n",
    "\n",
    "#AUC for y under sampled\n",
    "y_NB_pred_proba = gnbu.predict_proba(X_Models_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_Models_test,  y_NB_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_Models_test, y_NB_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "#Print Confusion Matrix\n",
    "cnf_matrix_over = metrics.confusion_matrix(y_Models_test, y_NB_pred_over_s)\n",
    "cnf_matrix_over\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_over), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix Over Sampled', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "#Print Confusion Matrix\n",
    "cnf_matrix_under = metrics.confusion_matrix(y_Models_test, y_NB_pred_under_s)\n",
    "cnf_matrix_under\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_under), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix under sampled', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conclusion - With accuracy of 81% for each y1 and y2, the Naive Bayes model is superior to the base model which had an accuracy of 71% for each target.\n",
    "#However, there is still room for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MD's Modeling Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Decision Tree Model (DT) Over Under Sampled\n",
    ">   <b> Assumption: </b> All features are usefull for Y1 & Y2!\n",
    "<br><b> Calculate: </b> How many times are you right?\n",
    "<br><b> Reason: </b> This can be set as the baseline for our accuracy of Y1 and Y2 - the computer model should at least beat this in order for it to be better than guessing.\n",
    "<br><b> Answer: </b> \n",
    "* <b> Y1 </b>: 85.56% accuracy\n",
    "* <b> Y2: </b> 85.45% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to take look at the models df to see if everything is correct\n",
    "df_Models.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Decision Tree classifer object\n",
    "clfo = DecisionTreeClassifier(criterion = \"entropy\", max_depth=3)\n",
    "clfu = DecisionTreeClassifier(criterion = \"entropy\", max_depth=3)\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clfo = clfo.fit(over_sampl_X_Models_train, over_sampled_y_Models_train)\n",
    "clfu = clfu.fit(under_sampled_X_Models_train, under_sampled_y_Models_train)\n",
    "\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_DT_pred_over = clfo.predict(X_Models_test)\n",
    "y_DT_pred_under = clfu.predict(X_Models_test) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_Models_test, y_DT_pred_over))\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_Models_test, y_DT_pred_under))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assess the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the Classification Report\n",
    "print(metrics.classification_report(y_Models_test, y_DT_pred_over))\n",
    "print(metrics.classification_report(y_Models_test, y_DT_pred_under))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the Confusion Matrix\n",
    "print(metrics.confusion_matrix(y_Models_test, y_DT_pred_over))\n",
    "print(metrics.confusion_matrix(y_Models_test, y_DT_pred_under))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUC for y\n",
    "y_DT_pred_proba_over = clfo.predict_proba(X_Models_test)[::,1]\n",
    "fpr_DT_over, tpr_DT_over, _ = metrics.roc_curve(y_Models_test,  y_DT_pred_proba_over)\n",
    "auc_DT_over = metrics.roc_auc_score(y_Models_test, y_DT_pred_proba_over)\n",
    "plt.plot(fpr_DT_over,tpr_DT_over,label=\"auc=\"+str(auc_DT_over))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "print(\"\")\n",
    "\n",
    "#AUC for y\n",
    "y_DT_pred_proba_under = clfu.predict_proba(X_Models_test)[::,1]\n",
    "fpr_DT_under, tpr_DT_under, _ = metrics.roc_curve(y_Models_test,  y_DT_pred_proba_under)\n",
    "auc_DT_under = metrics.roc_auc_score(y_Models_test, y_DT_pred_proba_under)\n",
    "plt.plot(fpr_DT_under,tpr_DT_under,label=\"auc=\"+str(auc_DT_under))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "print(\"\")\n",
    "\n",
    "#Print Confusion Matrix\n",
    "cnf_matrix_DT_over = metrics.confusion_matrix(y_Models_test, y_DT_pred_over)\n",
    "cnf_matrix_DT_over\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_DT_over), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix DT Over', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "#Print Confusion Matrix\n",
    "cnf_matrix_DT_under = metrics.confusion_matrix(y_Models_test, y_DT_pred_under)\n",
    "cnf_matrix_DT_under\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_DT_under), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix DT Under', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "85.45% is much better than the baseline model of 70%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Random Forest Model (RF) Over Under Sampled\n",
    ">   <b> Assumption: </b> All features are usefull for Y1 & Y2!\n",
    "<br><b> Calculate: </b> How many times are you right?\n",
    "<br><b> Reason: </b> This can be set as the baseline for our accuracy of Y1 and Y2 - the computer model should at least beat this in order for it to be better than guessing.\n",
    "<br><b> Answer: </b> \n",
    "* <b> Y1 </b>: 90% accuracy\n",
    "* <b> Y2: </b> 90% accuracy (not sure if Y2 result should be the same as Y1 but could make sense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Classifier\n",
    "#Import scikit-learn dataset library\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Random Forest Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "rfco = RandomForestClassifier(n_estimators=100)\n",
    "rfcu = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_Models_test)\n",
    "rfco.fit(over_sampl_X_Models_train, over_sampled_y_Models_train)\n",
    "rfcu.fit(under_sampled_X_Models_train, under_sampled_y_Models_train)\n",
    "\n",
    "y_RF_over_pred = rfco.predict(X_Models_test)\n",
    "y_RF_under_pred = rfcu.predict(X_Models_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of the Classification Report\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_Models_test, y_RF_over_pred))\n",
    "print(classification_report(y_Models_test, y_RF_under_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of the Confusion Matrix\n",
    "print(confusion_matrix(y_Models_test, y_RF_over_pred))\n",
    "print(confusion_matrix(y_Models_test, y_RF_under_pred))\n",
    "\n",
    "\n",
    "#AUC for y\n",
    "y_RF_over_pred_proba = rfco.predict_proba(X_Models_test)[::,1]\n",
    "fpr_RF_over, tpr_RF_over, _ = metrics.roc_curve(y_Models_test,  y_RF_over_pred_proba)\n",
    "auc_RF_over = metrics.roc_auc_score(y_Models_test, y_RF_over_pred_proba)\n",
    "plt.plot(fpr_RF_over,tpr_RF_over,label=\"auc=\"+str(auc_RF_over))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "print(\"\")\n",
    "\n",
    "#Print Confusion Matrix\n",
    "cnf_matrix_RF_over = metrics.confusion_matrix(y_Models_test, y_RF_over_pred) \n",
    "cnf_matrix_RF_over\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_RF_over), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix RF Over sampled', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of the Confusion Matrix\n",
    "print(confusion_matrix(y_Models_test, y_RF_under_pred))\n",
    "\n",
    "#AUC for y\n",
    "y_RF_under_pred_proba = rfcu.predict_proba(X_Models_test)[::,1]\n",
    "fpr_RF_under, tpr_RF_under, _ = metrics.roc_curve(y_Models_test,  y_RF_under_pred_proba)\n",
    "auc_RF_under = metrics.roc_auc_score(y_Models_test, y_RF_under_pred_proba)\n",
    "plt.plot(fpr_RF_under, tpr_RF_under,label=\"auc=\"+str(auc_RF_under))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "print(\"\")\n",
    "\n",
    "#Print Confusion Matrix\n",
    "cnf_matrix_RF_under = metrics.confusion_matrix(y_Models_test, y_RF_under_pred)\n",
    "cnf_matrix_RF_under\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_RF_under), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix RF under sampled', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 90% accuracy for both Random Forest Models, so far better than the Decision Tree Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More to do... Can do Feature Importance using scikit_learn - have the code for it but couldn't get it to run ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Attempt on the feature selection using scikit-learn\n",
    "# # import the package\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# #Create a Gaussian Classifier\n",
    "# clf=RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# #Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "# clf.fit(X_Models_train,y_Models_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# feature_imp = pd.Series(clf.feature_importances_,index=y_Models_test).sort_values(ascending=False)\n",
    "# feature_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Support Vector Machines (SVM) Classification Model (SVC) Over Under Sampled\n",
    ">   <b> Assumption: </b> All features are usefull for Y1 & Y2!\n",
    "<br><b> Calculate: </b> How many times are you right?\n",
    "<br><b> Reason: </b> This can be set as the baseline for our accuracy of Y1 and Y2 - the computer model should at least beat this in order for it to be better than guessing.\n",
    "<br><b> Answer: </b> \n",
    "* <b> Y1 </b>: 70.78% accuracy\n",
    "* <b> Y2: </b> 70.78% accuracy (not sure if Y2 result should be the same as Y1 but could make sense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###1. Step 1: Specify the Model\n",
    "from sklearn import svm\n",
    "svco = svm.SVC(kernel='rbf',cache_size=7000,gamma= 'auto', C=5, probability =True, degree = 5) # gamma= 0.001 , kernel='poly', 'rbf',‘linear’\n",
    "svcu = svm.SVC(kernel='rbf',cache_size=7000,gamma= 'auto', C=5, probability =True, degree = 5) # gamma= 0.001 , kernel='poly', 'rbf',‘linear’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###2. Step 2: Generate Test Data\n",
    "# X_train_svc, X_test_svc, y1_train_svc, y1_test_svc, y2_train_svc, y2_test_svc = train_test_split(X, y1,y2, test_size=0.3,random_state=1000) \n",
    "# X_Models_train, X_Models_test, y1_Models_train, y1_Models_test, y2_Models_train, y2_Models_test\n",
    "#Standard Scale the data to allow for better SVC model performance\n",
    "# scaler = StandardScaler().fit(X_Models_train) \n",
    "# standardized_X_svc = scaler.transform(X_Models_train) \n",
    "# standardized_X_test_svc = scaler.transform(X_Models_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###3. Step 3: Build the Model\n",
    "#Train the model using the training sets\n",
    "svco.fit(over_sampl_X_Models_train, over_sampled_y_Models_train)\n",
    "svcu.fit(under_sampled_X_Models_train, under_sampled_y_Models_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred_svc_over = svco.predict(X_Models_test)\n",
    "y_pred_svc_under = svcu.predict(X_Models_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###4. Step 4: Assess the Model\n",
    "print(\"Accuracy_svc_over:\",metrics.accuracy_score(y_Models_test, y_pred_svc_over))\n",
    "cnf_matrix_svc_over = metrics.confusion_matrix(y_Models_test, y_pred_svc_over)\n",
    "print(\"Accuracy_svc_under:\",metrics.accuracy_score(y_Models_test, y_pred_svc_under))\n",
    "cnf_matrix_svc_under = metrics.confusion_matrix(y_Models_test, y_pred_svc_under)\n",
    "\n",
    "\n",
    "\n",
    "print(cnf_matrix_svc_over)\n",
    "print(cnf_matrix_svc_under)\n",
    "\n",
    "\n",
    "\n",
    "#AUC for y1\n",
    "y_SVM_over_pred_proba = svco.predict_proba(X_Models_test)[::,1]\n",
    "fpr_SVM_over, tpr_SVM_over, _ = metrics.roc_curve(y_Models_test,  y_SVM_over_pred_proba)\n",
    "auc_SVM_over = metrics.roc_auc_score(y_Models_test, y_SVM_over_pred_proba)\n",
    "plt.plot(fpr_SVM_over, tpr_SVM_over,label=\"auc=\"+str(auc_SVM_over))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "print(\"\")\n",
    "\n",
    "#AUC for y1\n",
    "y_SVM_under_pred_proba = svcu.predict_proba(X_Models_test)[::,1]\n",
    "fpr_SVM_under,tpr_SVM_under, _ = metrics.roc_curve(y_Models_test,  y_SVM_under_pred_proba)\n",
    "auc_SVM_under = metrics.roc_auc_score(y_Models_test, y_SVM_under_pred_proba)\n",
    "plt.plot(fpr_SVM_under,tpr_SVM_under,label=\"auc=\"+str(auc_SVM_under))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "class_names_svc=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks_svc = np.arange(len(class_names_svc))\n",
    "plt.xticks(tick_marks_svc, class_names_svc)\n",
    "plt.yticks(tick_marks_svc, class_names_svc)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_svc_over), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix svc over sampled', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "class_names_svc=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks_svc = np.arange(len(class_names_svc))\n",
    "plt.xticks(tick_marks_svc, class_names_svc)\n",
    "plt.yticks(tick_marks_svc, class_names_svc)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_svc_under), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix svc under sampled', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "\n",
    "print(classification_report(y_Models_test, y_pred_svc))\n",
    "print(\"Accuracy_svc_over:\",metrics.accuracy_score(y_Models_test, y_pred_svc_over))\n",
    "\n",
    "print(classification_report(y_Models_test, y_pred_svc))\n",
    "print(\"Accuracy_svc_under:\",metrics.accuracy_score(y_Models_test, y_pred_svc_under))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. XGBoost Model (XGB) Over Under Sampled\n",
    ">   <b> Assumption: </b> XXXAll features are usefull for Y1 & Y2!XXX\n",
    "<br><b> Calculate: </b> How many times are you right?\n",
    "<br><b> Reason: </b> XXXThis can be set as the baseline for our accuracy of Y1 and Y2 - the computer model should at least beat this in order for it to be better than guessing.XXX\n",
    "<br><b> Answer: </b> \n",
    "* <b> Y2 </b>: 90.19% accuracy\n",
    "* <b> Y2: </b> XXX94.97 RMSEXXX - not sure this is right!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###0. Step 0: Import Needed Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Needed Packages\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Do preliminary work\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###1. Step 1: Specify the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate an XGBoost Classifer Model - for y1 (No_Sale)\n",
    "XGB_class_o = xgb.XGBClassifier(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 5, alpha = 10, n_estimators = 100)\n",
    "XGB_class_u = xgb.XGBClassifier(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 5, alpha = 10, n_estimators = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2. Steps 2-4: Generate Test Data, Build the Models & Assess the Models for y2 (Sale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put Data into structure for XGBoost- for y2 (Sale) \n",
    "\n",
    "#Train the model using the training sets for y1\n",
    "XGB_class_o.fit(over_sampl_X_Models_train, over_sampled_y_Models_train)\n",
    "XGB_class_u.fit(under_sampled_X_Models_train, under_sampled_y_Models_train)\n",
    "\n",
    "#Predict the response for test dataset for y1\n",
    "y_XGB_pred_over = XGB_class_o.predict(X_Models_test)\n",
    "y_XGB_pred_under = XGB_class_u.predict(X_Models_test)\n",
    "\n",
    "#Calculate RMSE for y2\n",
    "rmse_XGB_over = np.sqrt(mean_squared_error(y_Models_test, y_XGB_pred_over))\n",
    "print(\"XGBoost's RMSE for y2 is: %f\" % (rmse_XGB_over))\n",
    "rmse_XGB_under = np.sqrt(mean_squared_error(y_Models_test, y_XGB_pred_under))\n",
    "print(\"XGBoost's RMSE for y2 is: %f\" % (rmse_XGB_under))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Create error ratio to evaluate results for y2\n",
    "target_range_XGB = y_Models.max() - y_Models.min() # why Y_models\n",
    "print(\"XGB target range is: %f\" % (target_range_XGB))\n",
    "error_ratio_XGB = rmse_XGB/target_range_XGB\n",
    "print(\"XGBoost's Error Ratio for y2 is: %f\" % (error_ratio_XGB))\n",
    "\n",
    "target_range_XGB = y_Models.max() - y_Models.min()\n",
    "print(\"XGB target range is: %f\" % (target_range_XGB))\n",
    "error_ratio_XGB = rmse_XGB/target_range_XGB\n",
    "print(\"XGBoost's Error Ratio for y2 is: %f\" % (error_ratio_XGB))\n",
    "\n",
    "gnbo.fit(over_sampl_X_Models_train, over_sampled_y_Models_train)\n",
    "gnbu.fit(under_sampled_X_Models_train, under_sampled_y_Models_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ISSUE WITH THE ABOVE - HOW COME THE TARGET RANGE IS 1? DOES IT HAVE TO DO WITH 0/1 STATUS OF TARGET VARIABLE???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Accuracy, how often is the classifier correct?\n",
    "# Accuracy for y2\n",
    "print(\"y Accuracy Over:\",metrics.accuracy_score(y_Models_test, y_XGB_pred_over))\n",
    "print(\"\")\n",
    "print(\"y Accuracy Under:\",metrics.accuracy_score(y_Models_test, y_XGB_pred_under))\n",
    "print(\"\")\n",
    "\n",
    "#Can use classification report to assess model adequacy, too\n",
    "print(metrics.classification_report(y_Models_test, y_XGB_pred_over, labels=class_names))\n",
    "print(metrics.classification_report(y_Models_test, y_XGB_pred_under, labels=class_names))\n",
    "\n",
    "#AUC for y over sampled\n",
    "y_XGB_over_pred_proba = XGB_class_o.predict_proba(X_Models_test)[::,1]\n",
    "fpr_XGB_over, tpr_XGB_over, _ = metrics.roc_curve(y_Models_test,  y_XGB_over_pred_proba)\n",
    "auc_XGB_over = metrics.roc_auc_score(y_Models_test, y_XGB_over_pred_proba)\n",
    "plt.plot(fpr_XGB_over,tpr_XGB_over,label=\"auc=\"+str(auc_XGB_over))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "#AUC for y under sampled\n",
    "y_XGB_under_pred_proba = XGB_class_u.predict_proba(X_Models_test)[::,1]\n",
    "fpr_XGB_under, tpr_XGB_under, _ = metrics.roc_curve(y_Models_test,  y_XGB_under_pred_proba)\n",
    "auc_XGB_under = metrics.roc_auc_score(y_Models_test, y_XGB_under_pred_proba)\n",
    "plt.plot(fpr_XGB_under,tpr_XGB_under,label=\"auc=\"+str(auc_XGB_under))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "print(\"\")\n",
    "\n",
    "#Print Confusion Matrix\n",
    "cnf_matrix_XGB_over = metrics.confusion_matrix(y_Models_test, y_XGB_pred_over)\n",
    "cnf_matrix_XGB_over\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_XGB_over), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix XGB over sampled', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "#Print Confusion Matrix\n",
    "cnf_matrix_XGB_under = metrics.confusion_matrix(y_Models_test, y_XGB_pred_under)\n",
    "cnf_matrix_XGB_under\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_XGB_under), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix XGB under sampled', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More to go...including: k-fold cross-validation, visualization for feature importance & hyper-parameter tuning to improve model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F. Neural Network Model (NN) Over Under sampled\n",
    ">   <b> Assumption: </b> All features are usefull for Y1 & Y2!\n",
    "<br><b> Calculate: </b> How many times are you right?\n",
    "<br><b> Reason: </b> This can be set as the baseline for our accuracy of Y1 and Y2 - the computer model should at least beat this in order for it to be better than guessing.\n",
    "<br><b> Answer: </b> \n",
    "* <b> Y1 </b>: 90.26% accuracy\n",
    "* <b> Y2: </b> -% accuracy (not sure if Y2 result should be the same as Y1 but could make sense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_Models_train, X_Models_test, y1_Models_train, y1_Models_test, y2_Models_train, y2_Models_test\n",
    "\n",
    "\n",
    "###1. Step 1: Specify the Model\n",
    "# Import the model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Initializing the multilayer perceptron\n",
    "# mlp = MLPClassifier(hidden_layer_sizes = (3,1),solver='sgd',learning_rate_init= 0.01, max_iter=50)\n",
    "mlpo= MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto', beta_1=0.9, \n",
    "beta_2=0.999, early_stopping=False, epsilon=1e-08,       \n",
    "hidden_layer_sizes=(7,3), learning_rate='adaptive',      \n",
    "learning_rate_init=0.01, max_iter=10000, momentum=0.9,       \n",
    "nesterovs_momentum=True, power_t=0.5, random_state=1000,       \n",
    "shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,       \n",
    "verbose=False, warm_start=True)\n",
    "\n",
    "mlpu= MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto', beta_1=0.9, \n",
    "beta_2=0.999, early_stopping=False, epsilon=1e-08,       \n",
    "hidden_layer_sizes=(7,3), learning_rate='adaptive',      \n",
    "learning_rate_init=0.01, max_iter=10000, momentum=0.9,       \n",
    "nesterovs_momentum=True, power_t=0.5, random_state=1000,       \n",
    "shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,       \n",
    "verbose=False, warm_start=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###2. Step 2: Generate Test Data\n",
    "# Train the model\n",
    "mlpo.fit(over_sampl_X_Models_train, over_sampled_y_Models_train)\n",
    "mlpu.fit(under_sampled_X_Models_train, under_sampled_y_Models_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###3. Step 3: Build the Model\n",
    "y_pred_nn_over = mlpo.predict(X_Models_test)\n",
    "y_pred_nn_under = mlpu.predict(X_Models_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###4. Step 4: Assess the Model\n",
    "# Score takes a feature matrix X_test and the expected target values y_test. \n",
    "# Predictions for X_test are compared with y_test\n",
    "\n",
    "print(\"MLP over score is\",mlpo.score(X_Models_test,y_Models_test))\n",
    "print(\"MLP under score is\",mlpu.score(X_Models_test,y_Models_test))\n",
    "\n",
    "# Accuracy for y NN\n",
    "#print(\"y Accuracy NN:\",metrics.accuracy_score(y_Models_test, y_XGB_pred))\n",
    "#print(\"\")\n",
    "\n",
    "###4. Step 4: Assess the Model\n",
    "print(\"Accuracy_nn_over:\",metrics.accuracy_score(y_Models_test, y_pred_nn_over))\n",
    "cnf_matrix_nn_over = metrics.confusion_matrix(y_Models_test, y_pred_nn_over)\n",
    "\n",
    "print(cnf_matrix_nn_over)\n",
    "\n",
    "\n",
    "print(\"Accuracy_nn_under:\",metrics.accuracy_score(y_Models_test, y_pred_nn_under))\n",
    "cnf_matrix_nn_under = metrics.confusion_matrix(y_Models_test, y_pred_nn_under)\n",
    "\n",
    "print(cnf_matrix_nn_under)\n",
    "\n",
    "# Plot AOC over sampled\n",
    "y_pred_proba_nn_over = mlpo.predict_proba(X_Models_test)[::,1]\n",
    "fpr_nn_over, tpr_nn_over, _ = metrics.roc_curve(y_Models_test,  y_pred_proba_nn_over)\n",
    "auc_nn_over = metrics.roc_auc_score(y_Models_test, y_pred_proba_nn_over)\n",
    "plt.plot(fpr_nn_over, tpr_nn_over,label=\"data 1, auc=\"+str(auc_nn_over))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot AOC under sampled\n",
    "y_pred_proba_nn_under = mlpu.predict_proba(X_Models_test)[::,1]\n",
    "fpr_nn_under, tpr_nn_under, _ = metrics.roc_curve(y_Models_test,  y_pred_proba_nn_under)\n",
    "auc_nn_under = metrics.roc_auc_score(y_Models_test, y_pred_proba_nn_under)\n",
    "plt.plot(fpr_nn_under, tpr_nn_under,label=\"data 1, auc=\"+str(auc_nn_under))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "\n",
    "# over sampled\n",
    "class_names_nn=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks_nn = np.arange(len(class_names_nn))\n",
    "plt.xticks(tick_marks_nn, class_names_nn)\n",
    "plt.yticks(tick_marks_nn, class_names_nn)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_nn_over), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix NN over sampled', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "print(classification_report(y_Models_test, y_pred_nn_over))\n",
    "print(\"Accuracy_svc_over:\",metrics.accuracy_score(y_Models_test, y_pred_nn_over))\n",
    "\n",
    "# under sampled\n",
    "class_names_nn=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks_nn = np.arange(len(class_names_nn))\n",
    "plt.xticks(tick_marks_nn, class_names_nn)\n",
    "plt.yticks(tick_marks_nn, class_names_nn)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_nn_under), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix NN under sampled', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "print(classification_report(y_Models_test, y_pred_nn_under))\n",
    "print(\"Accuracy_svc_under:\",metrics.accuracy_score(y_Models_test, y_pred_nn_under))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model Over Under Sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">   <b> Assumption: </b> All features are usefull for Y\n",
    "<br><b> Calculate: </b> How many times are you right?\n",
    "<br><b> Reason: </b> This can be set as the baseline for our accuracy of Y - the computer model should at least beat this in order for it to be better than guessing.\n",
    "<br><b> Answer: </b> \n",
    "* <b> Y </b>: 90.3% accuracy - about the same as our other best scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the model\n",
    "# import the class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate the model (using the default parameters)\n",
    "logreg_over = LogisticRegression()\n",
    "logreg_under = LogisticRegression()\n",
    "\n",
    "# fit the model with data\n",
    "logreg_over.fit(over_sampl_X_Models_train, over_sampled_y_Models_train)\n",
    "logreg_under.fit(under_sampled_X_Models_train, under_sampled_y_Models_train)\n",
    "\n",
    "#\n",
    "y_LR_pred_over = logreg_over.predict(X_Models_test)\n",
    "y_LR_pred_under = logreg_under.predict(X_Models_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the metrics class for the Confusion Matrix\n",
    "from sklearn import metrics\n",
    "cnf_matrix_LogR_over = metrics.confusion_matrix(y_Models_test, y_LR_pred_over)\n",
    "print(cnf_matrix_LogR_over)\n",
    "\n",
    "cnf_matrix_LogR_under = metrics.confusion_matrix(y_Models_test, y_LR_pred_under)\n",
    "print(cnf_matrix_LogR_under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Confusion Matrix over sampled\n",
    "\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_LogR_over), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix Log Reg over', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Visualizing the Confusion Matrix under sampled\n",
    "\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_LogR_under), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix Log Reg under', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "y_LR_pred_proba_over = logreg_over.predict_proba(X_Models_test)[::,1]\n",
    "fpr_LR_over, tpr_LR_over, _ = metrics.roc_curve(y_Models_test,  y_LR_pred_proba_over)\n",
    "auc_LR_over = metrics.roc_auc_score(y_Models_test, y_LR_pred_proba_over)\n",
    "plt.plot(fpr_LR_over, tpr_LR_over,label=\"data 1, auc=\"+str(auc_LR_over))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "y_LR_pred_proba_under = logreg_under.predict_proba(X_Models_test)[::,1]\n",
    "fpr_LR_under, tpr_LR_under, _ = metrics.roc_curve(y_Models_test,  y_LR_pred_proba_under)\n",
    "auc_LR_under = metrics.roc_auc_score(y_Models_test, y_LR_pred_proba_under)\n",
    "plt.plot(fpr_LR_under, tpr_LR_under,label=\"data 1, auc=\"+str(auc_LR_under))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy over:\",metrics.accuracy_score(y_Models_test, y_LR_pred_over))\n",
    "print(\"Precision over:\",metrics.precision_score(y_Models_test, y_LR_pred_over))\n",
    "print(\"Recall over:\",metrics.recall_score(y_Models_test, y_LR_pred_over))\n",
    "\n",
    "print(\"Accuracy under:\",metrics.accuracy_score(y_Models_test, y_LR_pred_under))\n",
    "print(\"Precision under:\",metrics.precision_score(y_Models_test, y_LR_pred_under))\n",
    "print(\"Recall under:\",metrics.recall_score(y_Models_test, y_LR_pred_under))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#90.3% accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Model Over Under Sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_kmeans_over = np.array(over_sampl_X_Models_train)\n",
    "y_kmeans_over = np.array(over_sampled_y_Models_train)\n",
    "\n",
    "X_kmeans_under = np.array(under_sampled_X_Models_train)\n",
    "y_kmeans_under = np.array(under_sampled_y_Models_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "\n",
    "# load the model\n",
    "kmeans_over = KMeans(n_clusters=2, max_iter=600, algorithm = 'auto') # 2 clusters, sale or no sale\n",
    "kmeans_under = KMeans(n_clusters=2, max_iter=600, algorithm = 'auto') # 2 clusters, sale or no sale\n",
    "\n",
    "kmeans_over.fit(X_kmeans_over)\n",
    "kmeans_under.fit(X_kmeans_under)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "correct_over = 0\n",
    "for i in range(len(X_kmeans_over)):\n",
    "    predict_me_over = np.array(X_kmeans_over[i].astype(float))\n",
    "    predict_me_over = predict_me_over.reshape(-1, len(predict_me_over))\n",
    "    prediction_over = kmeans_over.predict(predict_me_over)\n",
    "    if prediction_over[0] == y_kmeans_over[i]:\n",
    "        correct_over += 1\n",
    "\n",
    "print(correct/len(X_kmeans_under))\n",
    "\n",
    "correct_under = 0\n",
    "for i in range(len(X_kmeans_under)):\n",
    "    predict_me_under = np.array(X_kmeans_under[i].astype(float))\n",
    "    predict_me_under = predict_me_under.reshape(-1, len(predict_me_under))\n",
    "    prediction_under = kmeans_under.predict(predict_me_under)\n",
    "    if prediction_under[0] == y_kmeans_under[i]:\n",
    "        correct_under += 1\n",
    "        \n",
    "print(correct/len(X_kmeans_under))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part V: Validation  <a name=\"part5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stratified Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skf = StratifiedKFold(n_splits=5, random_state=None)\n",
    "# # X is the feature set and y is the target\n",
    "# for train_index, test_index in skf.split(X,y): \n",
    "#     print(\"Train:\", train_index, \"Validation:\", val_index) \n",
    "#     X_train, X_test = X[train_index], X[val_index] \n",
    "#     y_train, y_test = y[train_index], y[val_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create some synthetic data for illustration\n",
    "# X_data = np.random.randint(5, size=(9, 2))\n",
    "# X_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular K-fold CV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kf = KFold(n_splits=3, random_state=2020)\n",
    "# for train_index, test_index in kf.split(X_data):\n",
    "#       print(\"Train:\")\n",
    "#       print(X_data[train_index])\n",
    "#       print(\"Test:\")\n",
    "#       print(X_data[test_index])\n",
    "#       print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeated K-fold CV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rkf = RepeatedKFold(n_splits=3, n_repeats=5, random_state=2020)\n",
    "# for train_i.ndex, test_index in rkf.split(X_data):\n",
    "#       print(\"Train:\")\n",
    "#       print(X_data[train_index])\n",
    "#       print(\"Test:\")\n",
    "#       print(X_data[test_index])\n",
    "#       print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Selection examples\n",
    "https://scikit-learn.org/stable/modules/grid_search.html#grid-search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part VI: Presentation/Visualization  <a name=\"part6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part VII: Sources  <a name=\"part7\"></a>\n",
    "1. https://i.ytimg.com/vi/CRKn-9gVNBw/maxresdefault.jpg\n",
    "2. https://support.google.com/analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part VIII: Next Steps (Discuss deployment, Lessons learned, Additional analyses had time permitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
